{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import libraries\n",
    "#from pyspark.sql.functions import avg\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams['font.size'] = 22\n",
    "import warnings    # To suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1 = pd.read_csv('PRED_31MAR17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df2 = pd.read_csv('PRED_30JUN17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df3 = pd.read_csv('PRED_31OCT17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df4 = pd.read_csv('PRED_31DEC17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df5 = pd.read_csv('PRED_31MAR18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df6 = pd.read_csv('PRED_30JUN18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df7 = pd.read_csv('PRED_31OCT18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df8 = pd.read_csv('PRED_31DEC18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df9 = pd.read_csv('PRED_MARR19.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "\n",
    "\n",
    "list_df = [df1,df2,df3,df4,df5,df6,df7,df8,df9]\n",
    "for i in list_df:\n",
    "  i = i.drop_duplicates().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quarter_dict = {3: 1, 6: 2, 10: 3, 12: 4} # quarter dict\n",
    "for i in list_df:\n",
    "    i['Year'] = i[i['EMP_STATUS'] == 'I']['HIRE_EXIT_DATE'].max().year\n",
    "    i['Quarter'] = pd.Series(i[i['EMP_STATUS'] == 'I']['HIRE_EXIT_DATE'].max().month).apply(quarter_dict.get)[0]\n",
    "    i['Month'] = i[i['EMP_STATUS'] == 'I']['HIRE_EXIT_DATE'].max().month\n",
    "for i in list_df:\n",
    "  i['ID']=i['Quarter'].astype(str)+'_'+i['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.concat([df1,df2,df3,df4,df5,df6,df7]).drop_duplicates().reset_index(drop=True)\n",
    "validation_set = pd.concat([df8,df9]).drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in list_df:\\n  i.replace(' ', np.nan, inplace=True)\\n  \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in list_df:\n",
    "  i.replace(' ', np.nan, inplace=True)\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=df1[df1.EMP_STATUS=='A']\n",
    "temp2=df2[df2.EMP_STATUS=='A']\n",
    "temp3=df3[df3.EMP_STATUS=='A']\n",
    "temp4=df4[df4.EMP_STATUS=='A']\n",
    "temp5=df5[df5.EMP_STATUS=='A']\n",
    "temp6=df6[df6.EMP_STATUS=='A']\n",
    "temp7=df7[df7.EMP_STATUS=='A']\n",
    "temp8=df8[df8.EMP_STATUS=='A']\n",
    "temp9=df9[df9.EMP_STATUS=='A']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(507392, 41)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MASK_EMPLOYEEID', 'EMPLOYEE_AGE', 'GENDER', 'NO_OF_LEAVES',\n",
       "       'TOTAL_REPORTEES', 'BAND', 'DEPARTMENT_ID', 'NO_OF_MANAGER_REPORTEES',\n",
       "       'TECHM_EXPERIENCE', 'TOTAL_EXPERIENCE', 'LAST_HIKE_PERCENT',\n",
       "       'MANAGER_RATING', 'NO_OF_PREVIOUS_EMPLOYERS', 'TENURE_LAST_EMPLOPYER',\n",
       "       'HIGHEST_EDUCATION', 'QUARTILE', 'LAST_PROMOTION_DATE',\n",
       "       'TOTAL_PROMOTIONS', 'MANAGER_LAST_PROMOTION', 'LAST_ONSITE_TRAVEL_DATE',\n",
       "       'LAST_ONSITE_TRAVEL_DAYS', 'REWARDS', 'PRIMARY_SKILL',\n",
       "       'SECONDARY_SKILL', 'BASE_LOCATION_CITY', 'CURRENT_LOCATION_CITY',\n",
       "       'HIRE_LOCATION_CITY', 'RESIGNATION_FLG', 'BILLABILITY_STATUS',\n",
       "       'CURRENT_RATING', 'LAST_RATING', 'ALL_PREVIOUS_RAINGS',\n",
       "       'LATEST_HIKE_PERCENT', 'OFFICERCODE_DESC', 'GRIEVANCE_FLG',\n",
       "       'EMP_STATUS', 'HIRE_EXIT_DATE', 'Year', 'Quarter', 'Month', 'ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['EMP_STATUS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[training_set['MASK_EMPLOYEEID']=='93578A9A4D51DBF7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd1=pd.merge(temp1[['MASK_EMPLOYEEID','EMP_STATUS']],df2,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd2=pd.merge(temp2[['MASK_EMPLOYEEID','EMP_STATUS']],df3,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd3=pd.merge(temp3[['MASK_EMPLOYEEID','EMP_STATUS']],df4,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd4=pd.merge(temp4[['MASK_EMPLOYEEID','EMP_STATUS']],df5,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd5=pd.merge(temp5[['MASK_EMPLOYEEID','EMP_STATUS']],df6,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd6=pd.merge(temp6[['MASK_EMPLOYEEID','EMP_STATUS']],df7,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd7=pd.merge(temp7[['MASK_EMPLOYEEID','EMP_STATUS']],df8,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd8=pd.merge(temp8[['MASK_EMPLOYEEID','EMP_STATUS']],df9,how='inner',on=['MASK_EMPLOYEEID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lisst_df = [fd8,fd8,fd8,fd8.fd8,fd8,fd8,fd8]\n",
    "for i in lisst_df:\n",
    "  \n",
    "    i['Leaves_avg'] = i[i['NO_OF_LEAVES'] == 'I']['HIRE_EXIT_DATE'].max().year\n",
    "    i['Quarter'] = pd.Series(i[i['EMP_STATUS'] == 'I']['HIRE_EXIT_DATE'].max().month).apply(quarter_dict.get)[0]\n",
    "    i['Month'] = i[i['EMP_STATUS'] == 'I']['HIRE_EXIT_DATE'].max().month'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_mar_17_target = df_mar_17_A.merge(df_jun_17_I[['MASK_EMPLOYEEID', 'EMP_STATUS']], on = ['MASK_EMPLOYEEID'], how = 'left')\n",
    "df_mar_17_target.rename(columns = {'EMP_STATUS_y': 'Target'}, inplace = True)\n",
    "df_mar_17_target['Target'] = df_mar_17_target['Target'].fillna('A')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Deletng EMP_STATUS column from original df\n",
    "for i in range(8):\n",
    "    data=globals()[\"fd\" + str(i + 1)]\n",
    "    data.drop(['EMP_STATUS_x'],axis=1,inplace=True)\n",
    "    data.rename(columns={'EMP_STATUS_y':'TARGET_VALUE'},inplace=True)\n",
    "    globals()[\"fd\" + str(i + 1)]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = df2.append(df3)\n",
    "f1 = df2.append(df4)\n",
    "triple1=pd.merge(temp1[['MASK_EMPLOYEEID','EMP_STATUS']],f1,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "f2 = df3.append(df4)\n",
    "f2 = df3.append(df5)\n",
    "triple2=pd.merge(temp2[['MASK_EMPLOYEEID','EMP_STATUS']],f2,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "f3 = df4.append(df5)\n",
    "f3 = df4.append(df6)\n",
    "triple3=pd.merge(temp3[['MASK_EMPLOYEEID','EMP_STATUS']],f3,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "f4 = df5.append(df6)\n",
    "f4 = df5.append(df7)\n",
    "triple4=pd.merge(temp4[['MASK_EMPLOYEEID','EMP_STATUS']],f4,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "\n",
    "f5 = df6.append(df7)\n",
    "f5 = df6.append(df8)\n",
    "triple5=pd.merge(temp5[['MASK_EMPLOYEEID','EMP_STATUS']],f5,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "\n",
    "list_f = [triple1,triple2,triple3,triple4,triple5]\n",
    "for i in list_f:\n",
    "  i['MASK_EMPLOYEEID'] = i['MASK_EMPLOYEEID'].drop_duplicates( keep='last', inplace=False)\n",
    "\n",
    "\n",
    "#triples['MASK_EMPLOYEEID'].drop_duplicates().reset_index(drop = True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deletng EMP_STATUS column from original df\n",
    "for i in range(5):\n",
    "    data=globals()[\"triple\" + str(i + 1)]\n",
    "    data.drop(['EMP_STATUS_x'],axis=1,inplace=True)\n",
    "    data.rename(columns={'EMP_STATUS_y':'TARGET_VALUE'},inplace=True)\n",
    "    globals()[\"triple\" + str(i + 1)]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df2.append(df3)\n",
    "y1 = df2.append(df4)\n",
    "year1=pd.merge(temp1[['MASK_EMPLOYEEID','EMP_STATUS']],y1,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "year1['MASK_EMPLOYEEID'] = year1['MASK_EMPLOYEEID'].drop_duplicates( keep='last', inplace=False)\n",
    "#year1 = year1.drop_duplicates(keep=False,inplace=True)\n",
    "\n",
    "y2 = df6.append(df7)\n",
    "y2 = df6.append(df8)\n",
    "year2=pd.merge(temp5[['MASK_EMPLOYEEID','EMP_STATUS']],y2,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "year2['MASK_EMPLOYEEID'] = year2['MASK_EMPLOYEEID'].drop_duplicates( keep='last', inplace=False)\n",
    "#year2 = year2.drop_duplicates(keep=False,inplace=True)\n",
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deletng EMP_STATUS column from original df\n",
    "for i in range(2):\n",
    "    data=globals()[\"year\" + str(i + 1)]\n",
    "    data.drop(['EMP_STATUS_x'],axis=1,inplace=True)\n",
    "    data.rename(columns={'EMP_STATUS_y':'TARGET_VALUE'},inplace=True)\n",
    "    globals()[\"year\" + str(i + 1)]=data\n",
    "    '''\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mapping completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "# print(fd1.QUARTILE.value_counts())\n",
    "for i in llist:\n",
    "  print('------------------')\n",
    "  print(i.QUARTILE.value_counts())\n",
    "  print((i[i.TARGET_VALUE=='I'].QUARTILE.value_counts()*100)/i.QUARTILE.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=fd1.QUARTILE.value_counts().index\n",
    "# colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\"]\n",
    "#explode=[0,0,0,0,0,0]\n",
    "sizes=(fd1[fd1.TARGET_VALUE=='I'].QUARTILE.value_counts()*100)/fd1.QUARTILE.value_counts().values\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\")\n",
    "plt.title(\"Quartile Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=fd1.QUARTILE.value_counts().index\n",
    "# colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\"]\n",
    "#explode=[0,0,0,0,0,0]\n",
    "sizes=i[i.TOTAL_EXPERIENCE<=3].QUARTILE.value_counts().values\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\")\n",
    "plt.title(\"Education Field Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1=pd.crosstab(data[\"QUARTILE\"],data['TARGET_VALUE'])\n",
    "var1.div(var1.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize = (10,5)) \n",
    "  #plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total percentage of experienced and inexperienced employee in each quartile for each quarter\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "l=[0.0,1.0,2.0,3.0,4.0]\n",
    "j=1\n",
    "for i in llist:\n",
    "  print('Quarter ' , str(j))\n",
    "  print('total percent of inexperienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE<=3].QUARTILE.value_counts()*100/i.QUARTILE.value_counts())\n",
    "  print('total percent of experienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE>3].QUARTILE.value_counts()*100/i.QUARTILE.value_counts())\n",
    "  j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of experienced and inexperienced people who are attriting\n",
    "\n",
    "\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "l=[0.0,1.0,2.0,3.0,4.0]\n",
    "j=1\n",
    "for i in llist:\n",
    "  print('Quarter ' , str(j))\n",
    "  print('total percent of inexperienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE<=3][i.TARGET_VALUE=='I'].QUARTILE.value_counts()*100/i[i.TOTAL_EXPERIENCE<=3].QUARTILE.value_counts())\n",
    "  print('total percent of experienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE>3][i.TARGET_VALUE=='I'].QUARTILE.value_counts()*100/i[i.TOTAL_EXPERIENCE>3].QUARTILE.value_counts())\n",
    "  print('-----------')\n",
    "  j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tech mahindra experience per quartile\n",
    "\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "l=[0.0,1.0,2.0,3.0,4.0]\n",
    "j=1\n",
    "for i in llist:\n",
    "  print(i[i.TECHM_EXPERIENCE<=2].QUARTILE.value_counts()*100/i.QUARTILE.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (fd1[fd1.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts()>3).sum(),fd1[fd1.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts()\n",
    "# fd1[fd1.TARGET_VALUE=='I'][fd1.TOTAL_EXPERIENCE>2].TOTAL_PROMOTIONS.value_counts(),fd1[fd1.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.mean()\n",
    "\n",
    "j=1\n",
    "for i in llist:\n",
    "    print('QUARTER NUMBER ',str(j))\n",
    "    print('the percentage of total inexperienced and experienced',i[i.TOTAL_EXPERIENCE<=2].TOTAL_EXPERIENCE.value_counts().sum()*100/i.TOTAL_EXPERIENCE.value_counts().sum(),i[i.TOTAL_EXPERIENCE>2].TOTAL_EXPERIENCE.value_counts().sum()*100/i.TOTAL_EXPERIENCE.value_counts().sum())\n",
    "    print('percent of the inexperienced who are leaving',i[i.TOTAL_EXPERIENCE<=3][i.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts().sum()*100/i[i.TOTAL_EXPERIENCE<=3].TOTAL_EXPERIENCE.value_counts().sum())\n",
    "    print('percent of the experienced who are leaving',i[i.TOTAL_EXPERIENCE<=3][i.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts().sum()*100/i[i.TOTAL_EXPERIENCE>3].TOTAL_EXPERIENCE.value_counts().sum())\n",
    "    print('-------------------')\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j=1\n",
    "for i in llist:\n",
    "  print('for QUARTER ',str(j))\n",
    "  print((i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE<=3].TOTAL_PROMOTIONS.value_counts()*100)/i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE<=3].TOTAL_PROMOTIONS.value_counts().sum())\n",
    "  print((i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE>3].TOTAL_PROMOTIONS.value_counts()*100)/i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE>3].TOTAL_PROMOTIONS.value_counts().sum())\n",
    "  print('-------------------------')\n",
    "  j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripplott(variable):\n",
    "  f,ax=plt.subplots(figsize=(18,18))\n",
    "  sb.set(rc={'figure.figsize':(16,20)})\n",
    "  sb.stripplot(x='TARGET_VALUE', y=variable, data=fd1, alpha=0.3, jitter=True);\n",
    "  # plt.plot()\n",
    "  display()\n",
    "  stripplott(\"TOTAL_EXPERIENCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(figsize=(18,18))\n",
    "sb.set(rc={'figure.figsize':(100,50)})\n",
    "# sb.countplot(x='BAND',data=fd3)\n",
    "sb.factorplot(x='TARGET_VALUE',col='TOTAL_PROMOTIONS',kind='count',data=fd1)\n",
    "# fd1.groupby('NO_OF_LEAVES').BAND.hist(alpha=0.6)\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(300, 100))\n",
    "# sb.kdeplot(fd1.loc[fd1['TARGET_VALUE'] == 'I', 'NO_OF_MANAGER_REPORTEES'], ax=ax[0], label='INACTIVE(0)')\n",
    "# sb.kdeplot(fd1.loc[fd1['TARGET_VALUE'] == 'A', 'NO_OF_MANAGER_REPORTEES'], ax=ax[0], label='ACTIVE(1)')\n",
    "# plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE - PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain original copy\n",
    "#data1 = validation_set.copy()\n",
    "data1 = training_set.copy()\n",
    "\n",
    "#validation_set = pd.concat([fd1,fd2,fd3,fd4,fd5,fd6]).drop_duplicates().reset_index(drop=True)\n",
    "#data = fd8[fd8.TARGET_VALUE=='I'].copy()\n",
    "#data1 = fd8.copy()\n",
    "#llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.rename(columns={'EMP_STATUS':'TARGET_VALUE'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['TARGET_VALUE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_df = data1.select_dtypes(include=['object']).copy()\n",
    "cat_df.head()\n",
    "#df2 = spark.createDataFrame(feature_to_use)\n",
    "\n",
    "display(cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data1['PROMOTION_FLAG']=data1.apply(lambda data1 : 1 if data1.LAST_PROMOTION_DATE.year>2016 and data1.LAST_PROMOTION_DATE.month<6 else 0,axis=1)\n",
    "'''for i in llist:\n",
    "    #fd1['PROMOTION_FLAG']=fd1.apply(lambda fd1 : 1 if fd1.LAST_PROMOTION_DATE.year>2016 and fd1.LAST_PROMOTION_DATE.month<6 else 0,axis=1)\n",
    "    i['PROMOTION_FLAG']=i.apply(lambda i : 1 if i.LAST_PROMOTION_DATE.year>2016 and i.LAST_PROMOTION_DATE.month<6 else 0,axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['NO_OF_LEAVES'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lllist = [df1['NO_OF_LEAVES'],df2['NO_OF_LEAVES'],df3['NO_OF_LEAVES'],df4['NO_OF_LEAVES'],df5['NO_OF_LEAVES'],df6['NO_OF_LEAVES']]\n",
    "lilist = [df1['Leaves_avg'],df2['Leaves_avg'],df3['Leaves_avg'],df4['Leaves_avg'],df5['Leaves_avg'],df6['Leaves_avg']]\n",
    "for i in lllist:\n",
    "  for j in lilist:\n",
    "    if lllist.index(i)=lilist.index(j):\n",
    "      #for i<index(i):\n",
    "        j = i - sum(i)/index(j)\n",
    "      \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.groupby('MASK_EMPLOYEEID').agg({'NO_OF_LEAVES':'sum','ID':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['avg'] = data1.groupby('MASK_EMPLOYEE_ID').data1[['NO_OF_LEAVES']].mean(axis=1)\n",
    "#data1.groupby('MASK_EMPLOYEEID').agg({'NO_OF_LEAVES':'sum','ID':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_fun(df):\n",
    "  df1 = data1[(data1['Quarter'] <= df['Quarter']) & (data1['Year'] == df['Year']) & (data1['MASK_EMPLOYEEID'] == df['MASK_EMPLOYEEID'])]\n",
    "  leave = df1['NO_OF_LEAVES'].sum() / df1['ID'].nunique()\n",
    "  avg_leave = df['NO_OF_LEAVES'] - leave\n",
    "  \n",
    "  return avg_leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_coroutine(print_on = 10):\n",
    "    print (\"Starting progress monitor\")\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        yield\n",
    "        iterations += 1\n",
    "        if (iterations % print_on == 0):\n",
    "            print (\"{} iterations done\".format(iterations))\n",
    "\n",
    "def percentage_coroutine(to_process, print_on_percent = 0.10):\n",
    "    print (\"Starting progress percentage monitor\")\n",
    "\n",
    "    processed = 0\n",
    "    count = 0\n",
    "    print_count = to_process*print_on_percent\n",
    "    while True:\n",
    "        yield\n",
    "        processed += 1\n",
    "        count += 1\n",
    "        if (count >= print_count):\n",
    "            count = 0\n",
    "            pct = (float(processed)/float(to_process))*100\n",
    "\n",
    "            print (\"{}% finished\".format(pct))\n",
    "\n",
    "def trace_progress(func, progress = None):\n",
    "    def callf(*args, **kwargs):\n",
    "        if (progress is not None):\n",
    "            progress.send(None)\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return callf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co1 = progress_coroutine()\n",
    "co2 = percentage_coroutine(len(data1))\n",
    "data1['Avg_leaves'] = data1.apply(trace_progress(leave_fun, progress = co1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df2['Leaves_avg'] = df2['NO_OF_LEAVES']-((df1['NO_OF_LEAVES']+df2['NO_OF_LEAVES'])/2)\n",
    "df.sum(axis = 1, skipna = True)\n",
    "\n",
    "mylist = [['name','party','city','0','0','1'],['name','party','city','1','1','1']]\n",
    "\n",
    "print sum([sum(map(int, item[3:])) for item in mylist])  # prints 4\n",
    "\n",
    "\n",
    "z = []\n",
    "for i in list_df:\n",
    "for index, row in i.NO_OF_LEAVES.iterrows():\n",
    "...     z.append(row.x + row.y)\n",
    "for i in list_df:\n",
    "  for j in list_df:\n",
    "  \n",
    "    if (list_df.index(j)) <= (list_df.index(i)):\n",
    "      x = (sum(j['NO_OF_LEAVES']))\n",
    "\n",
    "       #i['Leaves_avg'] = i['NO_OF_LEAVES']-((sum(j['NO_OF_LEAVES']))/(list_df.index(i)+1))#sum([sum(map(int, item[3:])) for item in mylist]) \n",
    "'''\n",
    "\n",
    "      \n",
    "      \n",
    "''' for j in lllist_df:\n",
    "        if index(j)<=index(i):\n",
    "          i = i - sum(j)/index(i)'''\n",
    "'''df1['Leaves_avg'] = df1['NO_OF_LEAVES']\n",
    "df2['Leaves_avg'] = df2['NO_OF_LEAVES'] - ((df1['NO_OF_LEAVES']+df2['NO_OF_LEAVES'])/2)\n",
    "df3['Leaves_avg'] = df3['NO_OF_LEAVES'] - ((df1['NO_OF_LEAVES']+df2['NO_OF_LEAVES']+df3['NO_OF_LEAVES'])/3)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Avg_leaves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Leaves_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['NO_OF_LEAVES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2=fd1.copy()\n",
    "#data2.info()\n",
    "data1['CITY_FLAG']=data1.apply(lambda data1 : 1 if data1.HIRE_LOCATION_CITY==data1.CURRENT_LOCATION_CITY else 0,axis=1 )\n",
    "\n",
    "'''for i in llist:\n",
    "    i['CITY_FLAG']=i.apply(lambda i : 1 if i.HIRE_LOCATION_CITY==i.CURRENT_LOCATION_CITY else 0,axis=1 )'''\n",
    "#data1.CITY_FLAG.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data1.PRIMARY_SKILL.fillna(data1.SECONDARY_SKILL.str[0],inplace=True)\n",
    "data[\"PRIMARY_SKILL\"]= data[\"PRIMARY_SKILL\"].str.replace(np.nan, \"New Boston\", case = False)\n",
    "data[\"PRIMARY_SKILL\"]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.CURRENT_RATING.fillna(data1.LAST_RATING,inplace=True)\n",
    "'''for i in llist:\n",
    "    i.CURRENT_RATING.fillna(i.LAST_RATING,inplace=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.CURRENT_RATING.fillna(data1.ALL_PREVIOUS_RAINGS.str[-1],inplace=True)\n",
    "'''for i in llist:\n",
    "    i.CURRENT_RATING.fillna(i.ALL_PREVIOUS_RAINGS.str[-1],inplace=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"band.pickle\",\"rb\")\n",
    "dict_band = pickle.load(pickle_in)\n",
    "#print(dict_band)\n",
    "\n",
    "pickle_in = open(\"education.pickle\",\"rb\")\n",
    "dict_education = pickle.load(pickle_in)\n",
    "#print(dict_education)\n",
    "\n",
    "pickle_in = open(\"rating.pickle\",\"rb\")\n",
    "dict_rating = pickle.load(pickle_in)\n",
    "#print(dict_rating)\n",
    "\n",
    "pickle_in = open(\"education_name.pickle\",\"rb\")\n",
    "dict_edu_name = pickle.load(pickle_in)\n",
    "#print(dict_edu_name)\n",
    "\n",
    "pickle_in = open(\"gender.pickle\",\"rb\")\n",
    "dict_gender = pickle.load(pickle_in)\n",
    "#print(dict_gender)\n",
    "\n",
    "pickle_in = open(\"generic.pickle\",\"rb\")\n",
    "dict_generic = pickle.load(pickle_in)\n",
    "#print(dict_generic)\n",
    "\n",
    "pickle_in = open(\"target.pickle\",\"rb\")\n",
    "dict_target = pickle.load(pickle_in)\n",
    "#print(dict_target)\n",
    "\n",
    "#print(example_dict[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''quarter_dict = {3: 1, 6: 2, 10: 3, 12: 4} # quarter dict\n",
    "data1['Year'] = data1[data1['TARGET_VALUE'] == 'I']['HIRE_EXIT_DATE'].max().year\n",
    "data1['Quarter'] = pd.Series(data1[data1['TARGET_VALUE'] == 'I']['HIRE_EXIT_DATE'].max().month).apply(quarter_dict.get)[0]\n",
    "data1['Month'] = data1[data1['TARGET_VALUE'] == 'I']['HIRE_EXIT_DATE'].max().month\n",
    "\n",
    "data1['ID']=data1['Quarter'].astype(str)+'_'+data1['Year'].astype(str)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['HIGHEST_EDUCATION'].replace(dict_edu_name, inplace=True)\n",
    "'''for i in llist:\n",
    "    i['HIGHEST_EDUCATION'].replace(dict_edu_name, inplace=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['BAND'].replace(dict_band, inplace=True)\n",
    "data1['CURRENT_RATING'].replace(dict_rating, inplace=True)\n",
    "data1['LAST_RATING'].replace(dict_rating, inplace=True)\n",
    "data1['MANAGER_RATING'].replace(dict_rating, inplace=True)\n",
    "data1['HIGHEST_EDUCATION'].replace(dict_education, inplace=True)\n",
    "data1['GENDER'].replace(dict_gender, inplace=True)\n",
    "data1['GRIEVANCE_FLG'].replace(dict_generic, inplace=True)\n",
    "data1['BILLABILITY_STATUS'].replace(dict_generic, inplace=True)\n",
    "data1['RESIGNATION_FLG'].replace(dict_generic, inplace=True)\n",
    "data1['TARGET_VALUE'].replace(dict_target, inplace=True)\n",
    "\n",
    "'''\n",
    "for i in llist:\n",
    "    i['BAND'].replace(dict_band, inplace=True)\n",
    "    i['CURRENT_RATING'].replace(dict_rating, inplace=True)\n",
    "    i['LAST_RATING'].replace(dict_rating, inplace=True)\n",
    "    i['MANAGER_RATING'].replace(dict_rating, inplace=True)\n",
    "    i['HIGHEST_EDUCATION'].replace(dict_education, inplace=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion of object to category \n",
    "for i in data1:\n",
    "    if data1[i].dtype==object:\n",
    "        data1[i] = data1[i].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llisst = ['EMPLOYEE_AGE','NO_OF_LEAVES','TOTAL_REPORTEES','NO_OF_MANAGER_REPORTEES','TECHM_EXPERIENCE','TOTAL_EXPERIENCE','NO_OF_PREVIOUS_EMPLOYERS','LAST_ONSITE_TRAVEL_DAYS','TOTAL_PROMOTIONS','MANAGER_RATING','CURRENT_RATING','LAST_RATING','REWARDS','QUARTILE','TENURE_LAST_EMPLOPYER','LATEST_HIKE_PERCENT','LAST_HIKE_PERCENT','HIGHEST_EDUCATION','CITY_FLAG','Month','GRIEVANCE_FLG','BILLABILITY_STATUS']\n",
    "for j in llisst:\n",
    "      data1[j] = data1[j].fillna(0.0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corr = data1.corr()\n",
    "f,ax=plt.subplots(figsize=(10,10))\n",
    "#b.heatmap(corr,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".1f\",ax=ax)\n",
    "sb.heatmap(corr,annot=False,linewidths=0.5,linecolor=\"green\",fmt=\".1f\",ax=ax)\n",
    "#plt.show()\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#data1 = pd.get_dummies(data1, columns=['carrier'], prefix = ['carrier'])\n",
    "data1 = pd.get_dummies(data1, columns=['GENDER','DEPARTMENT_ID','PRIMARY_SKILL','SECONDARY_SKILL','CURRENT_LOCATION_CITY','BASE_LOCATION_CITY','HIRE_LOCATION_CITY','RESIGNATION_FLG','BILLABILITY_STATUS','ALL_PREVIOUS_RAINGS','OFFICERCODE_DESC','GRIEVANCE_FLG'], prefix = ['GENDER','DEPARTMENT_ID','PRIMARY_SKILL','SECONDARY_SKILL','CURRENT_LOCATION_CITY','BASE_LOCATION_CITY','HIRE_LOCATION_CITY','RESIGNATION_FLG','BILLABILITY_STATUS','ALL_PREVIOUS_RAINGS','OFFICERCODE_DESC','GRIEVANCE_FLG'])\n",
    "\n",
    "print(data1.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to put target variable in last\n",
    "dft = data1.pop('TARGET_VALUE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['TARGET_VALUE']=dft # add b series as a 'new' column.\n",
    "#df['x']=df2 # add b series as a 'new' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dft = data1.pop('HIRE_EXIT_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['HIRE_EXIT_DATE']=dft\n",
    "\n",
    "\n",
    "dft = data1.pop('LAST_PROMOTION_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['LAST_PROMOTION_DATE']=dft\n",
    "\n",
    "dft = data1.pop('MANAGER_LAST_PROMOTION') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['MANAGER_LAST_PROMOTION']=dft\n",
    "\n",
    "\n",
    "dft = data1.pop('LAST_ONSITE_TRAVEL_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['LAST_ONSITE_TRAVEL_DATE']=dft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing irrelevant features\n",
    "\n",
    "threshold = 90\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = (data1.corr().abs())*100\n",
    "corr_matrix.head(30)\n",
    "\n",
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()\n",
    "\n",
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))\n",
    "print(to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove = ['LAST_ONSITE_TRAVEL_DATE','MANAGER_LAST_PROMOTION','LAST_PROMOTION_DATE','HIRE_EXIT_DATE','TARGET_VALUE']\n",
    "remove = ['LAST_ONSITE_TRAVEL_DATE','MANAGER_LAST_PROMOTION','LAST_PROMOTION_DATE','HIRE_EXIT_DATE','RESIGNATION_FLG','CURRENT_LOCATION_CITY','Month','HIRE_LOCATION_CITY']\n",
    "\n",
    "data1.drop(remove, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "data1.to_pickle('preprocessed.pkl')    #to save the dataframe, df to 123.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "data1.to_pickle('test_valid.pkl')  '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PRE processing completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('Minst_Lstm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic statistics of categorical features\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  print(i.describe(include=[np.object]))\n",
    "#cat_col_names = data.select_dtypes(include=[np.object]).columns.tolist() # Get categorical feature names\n",
    "#cat_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attrition Target Variable Distribution\n",
    "lllist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in lllist:\n",
    "  attrition_freq = i[['TARGET_VALUE']].apply(lambda x: x.value_counts())\n",
    "  attrition_freq['frequency_percent'] = round((100 * attrition_freq / attrition_freq.sum()),2)\n",
    "\n",
    "  print(attrition_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(variable):\n",
    "  for i in llist:\n",
    "      print(i[variable].value_counts(),(i[i.TARGET_VALUE=='I'][variable].value_counts()*100)/i[variable].value_counts(),i[i.TARGET_VALUE=='I'][variable].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution('DEPARTMENT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  print(distribution('OFFICERCODE_DESC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"To understand how each features are impacting the attrition indicator, we need to first handle the null / missing values, otherwise our observations might not be accurate and will lead to wrong conclusions.\n",
    "\"\"\"\n",
    "for i in llist:\n",
    "    null_feat_df = pd.DataFrame()\n",
    "    null_feat_df['Null Count'] = i.isnull().sum().sort_values(ascending=False)\n",
    "    null_feat_df['Null Pct'] = null_feat_df['Null Count'] / float(len(i))\n",
    "\n",
    "    null_feat_df = null_feat_df[null_feat_df['Null Pct'] > 0]\n",
    "\n",
    "\n",
    "    total_null_feats = null_feat_df.shape[0]\n",
    "    null_feat_names = null_feat_df.index\n",
    "    print('Total number of features having null values: ', total_null_feats)\n",
    "    del null_feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling with 0\n",
    "for i in llist:\n",
    "    i[\"LATEST_HIKE_PERCENT\"].fillna(\"0\", inplace = True) \n",
    "    i[\"LAST_HIKE_PERCENT\"].fillna(\"0\", inplace = True)\n",
    "\n",
    "    i['EMPLOYEE_AGE'].fillna(i['EMPLOYEE_AGE'].mode()[0], inplace=True) \n",
    "#EMPLOYEE_AGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1) \n",
    "plt.subplot(221) \n",
    "data['GENDER'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender') \n",
    "plt.subplot(222) \n",
    "data['BAND'].value_counts(normalize=True).plot.bar(title= 'Band') \n",
    "plt.subplot(223) \n",
    "data['REWARDS'].value_counts(normalize=True).plot.bar(title= 'REWARDS') \n",
    "plt.subplot(224) \n",
    "#plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregating data set based on Attrition value\n",
    "\n",
    "#Attrition across Job Roles\n",
    "fig, ax6 = plt.subplots(1,2, figsize=(24,10))\n",
    "attr_yes = data[data.TARGET_VALUE=='I'] #subset with Attrition = Yes\n",
    "attr_no = data[data.TARGET_VALUE=='A'] ##subset with Attrition = No\n",
    "sb.countplot(x='BAND', hue='GENDER', data = attr_yes, palette=\"Set3\", ax = ax6[1])\n",
    "sb.countplot(x='BILLABILITY_STATUS', hue='GENDER', data = attr_yes,palette=\"Set3\", ax = ax6[0])\n",
    "plt.show()\n",
    "display()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def plot_count(feature, title,size=1):\n",
    "    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n",
    "    total = float(len(data))\n",
    "    g = sb.countplot(data[feature], order = data[feature].value_counts(normalize=True).index[:20], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "    plt.show()   \n",
    "    display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('CURRENT_RATING','current rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "plt.figure(figsize=(15, 8))\n",
    "cols = ['LAST_ONSITE_TRAVEL_DAYS', 'REWARDS', 'TOTAL_PROMOTIONS', 'TENURE_LAST_EMPLOPYER', 'NO_OF_PREVIOUS_EMPLOYERS']\n",
    "uniques = [len(data[col].unique()) for col in cols]\n",
    "sb.set(font_scale=1.2)\n",
    "sb.color_palette(\"Blues\")\n",
    "ax = sb.barplot(cols, uniques,log=True)\n",
    "ax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 10,\n",
    "            uniq,\n",
    "            ha=\"center\") \n",
    "# for col, uniq in zip(cols, uniques):\n",
    "#     ax.text(col, uniq, uniq, color='black', ha=\"center\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divisionplot(variable):\n",
    "  var=pd.crosstab(data[variable],data['QUARTILE'])\n",
    "  var.div(var.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize = (10,4)) \n",
    "  #plt.show()\n",
    "  display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "divisionplot('BAND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sb.jointplot(x=\"TOTAL_PROMOTIONS\", y=\"TOTAL_EXPERIENCE\", data=data,  ratio=3, color=\"r\")\n",
    "plt.show()\n",
    "display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_data = fd4.copy()\n",
    "listdf = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in listdf:\n",
    "  print(i['CURRENT_RATING'].value_counts() , i['CURRENT_RATING'][i['TARGET_VALUE'] == 'I'].value_counts())\n",
    "  #print(i['CURRENT_RATING'][i['TARGET_VALUE'] == 'I'].value_counts())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['CURRENT_RATING']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['CURRENT_RATING']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AGE_Interval'] = pd.cut(data['EMPLOYEE_AGE'], 6, labels=['19-25','25-35', '35-45', '45-55', '55-65','65+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age=pd.DataFrame(data.groupby(\"AGE_Interval\")[[\"TOTAL_REPORTEES\",\"NO_OF_LEAVES\",\"NO_OF_MANAGER_REPORTEES\",\"TECHM_EXPERIENCE\",\"TOTAL_EXPERIENCE\",\"LAST_HIKE_PERCENT\",\"NO_OF_PREVIOUS_EMPLOYERS\",\"TENURE_LAST_EMPLOPYER\",\"TOTAL_PROMOTIONS\",\"LAST_ONSITE_TRAVEL_DAYS\",\"REWARDS\",\"LATEST_HIKE_PERCENT\",\"HIGHEST_EDUCATION\"]].mean())\n",
    "age[\"Count\"]=data.AGE_Interval.value_counts(dropna=False)\n",
    "age.reset_index(level=0, inplace=True)\n",
    "age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax=sb.barplot(x=age.AGE_Interval,y=age.Count)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Age Counts\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['AGE_Interval'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax=sb.barplot(x=age.AGE_Interval,y=age.TOTAL_PROMOTIONS,palette = sb.cubehelix_palette(len(age.index)))\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"TOTAL_PROMOTIONS\")\n",
    "plt.title(\"TOTAL_PROMOTIONS According to Age\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "labels=data.AGE_Interval.value_counts().index\n",
    "# colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\"]\n",
    "#explode=[0,0,0,0,0,0]\n",
    "sizes=(data[data.TARGET_VALUE=='I'].AGE_Interval.value_counts()*100)/data.AGE_Interval.value_counts().values\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\")\n",
    "plt.title(\"AGE interval Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age1=pd.DataFrame(data.groupby(\"EMPLOYEE_AGE\")[[\"TOTAL_REPORTEES\",\"NO_OF_LEAVES\",\"NO_OF_MANAGER_REPORTEES\",\"TECHM_EXPERIENCE\",\"TOTAL_EXPERIENCE\",\"LAST_HIKE_PERCENT\",\"NO_OF_PREVIOUS_EMPLOYERS\",\"TENURE_LAST_EMPLOPYER\",\"TOTAL_PROMOTIONS\",\"LAST_ONSITE_TRAVEL_DAYS\",\"REWARDS\",\"LATEST_HIKE_PERCENT\",\"HIGHEST_EDUCATION\"]].mean())\n",
    "age1[\"Count\"]=data.EMPLOYEE_AGE.value_counts(dropna=False)\n",
    "age1.reset_index(level=0, inplace=True)\n",
    "age1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenorm=age1.apply(lambda x: x/max(x))\n",
    "agenorm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageb=data.EMPLOYEE_AGE.value_counts().index\n",
    "f,ax=plt.subplots(figsize=(15,15))\n",
    "sb.pointplot(y=agenorm.LAST_HIKE_PERCENT,x=ageb,color=\"purple\",alpha=0.8)\n",
    "sb.pointplot(y=agenorm.TOTAL_PROMOTIONS,x=ageb,color=\"sandybrown\",alpha=0.8)\n",
    "plt.text(5,0.65,\"LAST_HIKE_PERCENT\",color=\"purple\",fontsize=15,style=\"italic\")\n",
    "plt.text(5,0.63,\"TOTAL_PROMOTIONS\",color=\"sandybrown\",fontsize=15,style=\"italic\")\n",
    "plt.xlabel(\"Age\",fontsize=15,color=\"darkred\")\n",
    "plt.ylabel(\"Values\",fontsize=15,color=\"darkred\")\n",
    "plt.title(\"LAST_HIKE_PERCENTe VS JTOTAL_PROMOTIONS\",fontsize=15,color=\"darkred\")\n",
    "plt.grid()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageb=data.EMPLOYEE_AGE.value_counts().index\n",
    "f,ax=plt.subplots(figsize=(15,15))\n",
    "sb.pointplot(y=agenorm.LATEST_HIKE_PERCENT,x=ageb,color=\"purple\",alpha=0.8)\n",
    "sb.pointplot(y=agenorm.TOTAL_PROMOTIONS,x=ageb,color=\"sandybrown\",alpha=0.8)\n",
    "plt.text(5,0.65,\"LATEST_HIKE_PERCENT\",color=\"purple\",fontsize=15,style=\"italic\")\n",
    "plt.text(5,0.63,\"TOTAL_PROMOTIONS\",color=\"sandybrown\",fontsize=15,style=\"italic\")\n",
    "plt.xlabel(\"Age\",fontsize=15,color=\"darkred\")\n",
    "plt.ylabel(\"Values\",fontsize=15,color=\"darkred\")\n",
    "plt.title(\"LAST_HIKE_PERCENTe VS JTOTAL_PROMOTIONS\",fontsize=15,color=\"darkred\")\n",
    "plt.grid()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "sb.kdeplot(agenorm.REWARDS,agenorm.LAST_ONSITE_TRAVEL_DAYS,shade=True,cut=1)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "sb.kdeplot(agenorm.TOTAL_PROMOTIONS,agenorm.LAST_ONSITE_TRAVEL_DAYS,shade=True,cut=1)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promotions=pd.DataFrame(data.groupby(\"BAND\").TOTAL_PROMOTIONS.mean().sort_values(ascending=False))\n",
    "plt.figure(figsize=(5,5))\n",
    "ax=sb.barplot(x=promotions.index,y=promotions.TOTAL_PROMOTIONS)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"BAND\")\n",
    "plt.ylabel(\"TOTAL_PROMOTIONS\")\n",
    "plt.title(\"BAND with TOTAL_PROMOTIONS\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band=pd.DataFrame(data.groupby(\"BAND\")[\"EMPLOYEE_AGE\",\"NO_OF_LEAVES\",\"NO_OF_PREVIOUS_EMPLOYERS\",\"NO_OF_MANAGER_REPORTEES\",\"LAST_ONSITE_TRAVEL_DAYS\",\"LATEST_HIKE_PERCENT\",\"LAST_HIKE_PERCENT\",\"NO_OF_MANAGER_REPORTEES\",\"TENURE_LAST_EMPLOPYER\",\"TECHM_EXPERIENCE\",\"TOTAL_EXPERIENCE\"].mean())\n",
    "band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (9,10))\n",
    "sb.barplot(x=band.LATEST_HIKE_PERCENT,y=band.index,color='green',alpha = 0.5,label='LATEST_HIKE_PERCENT' )\n",
    "sb.barplot(x=band.LAST_ONSITE_TRAVEL_DAYS,y=band.index,color='blue',alpha = 0.7,label='LAST_ONSITE_TRAVEL_DAYS')\n",
    "sb.barplot(x=band.TECHM_EXPERIENCE,y=band.index,color='cyan',alpha = 0.6,label='Years At Company')\n",
    "sb.barplot(x=band.NO_OF_PREVIOUS_EMPLOYERS,y=band.index,color='yellow',alpha = 0.6,label='NO_OF_PREVIOUS_EMPLOYERS')\n",
    "sb.barplot(x=band.NO_OF_LEAVES,y=band.index,color='red',alpha = 0.6,label='NO_OF_LEAVES')\n",
    "\n",
    "ax.legend(loc='right',frameon = True)     \n",
    "ax.set(xlabel='Values', ylabel='band',title = \"band with Different Features\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.pairplot(data, vars=[\"TOTAL_PROMOTIONS\", \"REWARDS\"],hue=\"BAND\",size=5)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "sb.boxplot(x=\"GENDER\",y=\"EMPLOYEE_AGE\",hue=\"QUARTILE\",data=data,palette=\"Paired\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "ax.legend(frameon=False, loc='lower center', ncol=2)\n",
    "\n",
    "sb.boxplot(x=\"GENDER\",y=\"EMPLOYEE_AGE\",hue=\"BAND\" ,data=data,palette=\"hls\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "sb.pointplot(x=\"GENDER\", y=\"TECHM_EXPERIENCE\", hue=\"TARGET_VALUE\", data=data,\n",
    "              palette={\"I\": \"blue\", \"A\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"]);\n",
    "display()\n",
    "## Inference\n",
    "# Males and Females within the ranges for 'TotalWorkingYears' of 11 to 13 are less likely to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TECHM_Interval'] = pd.cut(data['TECHM_EXPERIENCE'], 6, labels=['0-5','5-10', '10-15', '15-20', '20-25','25+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "#plt.subplots(figsize=(15,5))\n",
    "sb.countplot(data.TECHM_Interval)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['HIGHEST_EDUCATION'].value_counts()\n",
    "\n",
    "\n",
    "#print(data['HIGHEST_EDUCATION'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['EDUCATION_CATEGORIES'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (11,5))\n",
    "#plt.subplots(figsize=(15,5))\n",
    "sb.countplot(data.EDUCATION_CATEGORIES)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data.EDUCATION_CATEGORIES.value_counts().index\n",
    "colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\",\"b\"]\n",
    "explode=[1,0,0,0,1,1]\n",
    "sizes=data.EDUCATION_CATEGORIES.value_counts() \n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,colors=colors,autopct=\"%1.1f%%\")\n",
    "plt.title(\"Education Field Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  i['LeavesInterval'] = pd.cut(i['NO_OF_LEAVES'][i['NO_OF_LEAVES']<50], 5, labels=['<10', '<20', '<30', '<40','<50+'])\n",
    "  freq = i[['LeavesInterval']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['LeavesInterval']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['LeavesInterval'].value_counts()\n",
    "data['LeavesInterval'] = pd.cut(i['NO_OF_LEAVES'][i['NO_OF_LEAVES']<50], 5, labels=['<10', '<20', '<30', '<40','<50+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['DEPARTMENT_ID']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['DEPARTMENT_ID']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  #print(freq)\n",
    "  print(freq[\"frequency_percent\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library and dataset\n",
    "#import seaborn as sns\n",
    "#df = sns.load_dataset('iris')\n",
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "# plot of 2 variables\n",
    "sb.kdeplot(data['TOTAL_REPORTEES'], shade=True, color=\"r\")\n",
    "sb.kdeplot(data['NO_OF_MANAGER_REPORTEES'], shade=True, color=\"b\")\n",
    "#sns.plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library and dataset\n",
    "#import seaborn as sns\n",
    "#df = sns.load_dataset('iris')\n",
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "# plot of 2 variables\n",
    "sb.kdeplot(data['TOTAL_PROMOTIONS'], shade=True, color=\"r\")\n",
    "sb.kdeplot(data['REWARDS'], shade=True, color=\"b\")\n",
    "#sns.plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['HIRE_LOCATION_CITY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = pd.Index(data.CURRENT_LOCATION_CITY)\n",
    "idx2 = pd.Index(data.BASE_LOCATION_CITY)\n",
    "idx3 = pd.Index(data.HIRE_LOCATION_CITY)\n",
    "#main_data['unique'] = idx1.difference(idx2).values\n",
    "print(idx1.difference(idx2).values)\n",
    "print(idx2.difference(idx1).values)\n",
    "print(idx3.difference(idx1).values)\n",
    "#print(idx3.difference(idx2).values)\n",
    "#array([3, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['HIRE_LOCATION_CITY']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['HIRE_LOCATION_CITY']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['HIRE_EXIT_DATE']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['HIRE_EXIT_DATE']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['LeavesInterval','TECHM_Interval','CURRENT_RATING','AGE_Interval', 'TOTAL_PROMOTIONS','DEPARTMENT_ID ','QUARTILE']\n",
    "fig=plt.subplots(figsize=(10,15))\n",
    "for i, j in enumerate(features):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.subplots_adjust(hspace = 1.0)\n",
    "    sb.countplot(x=j,data = data)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"No. of employee\")\n",
    "    display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart of workers\n",
    "labels = ['M', 'F']\n",
    "sizes = [data['GENDER'].value_counts()[0],\n",
    "         data['GENDER'].value_counts()[1]\n",
    "        ]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True)\n",
    "ax1.axis('equal')\n",
    "plt.title('Gender Pie Chart', fontsize=20)\n",
    "plt.show()\n",
    "display()\n",
    "\n",
    "## Inference\n",
    "# Proportion of male is higher than female associates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.subplots(figsize=(5,5))\n",
    "sb.factorplot(data=data,y='TOTAL_PROMOTIONS',x='BAND',size=7,aspect=2,kind='point')\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plotting the KDEplots\n",
    "f, axes = plt.subplots(3, 3, figsize=(10, 8), \n",
    "                       sharex=False, sharey=False)\n",
    "\n",
    "# Defining our colormap scheme\n",
    "s = np.linspace(0, 3, 7)\n",
    "cmap = sb.cubehelix_palette(start=0.0, light=1, as_cmap=True)\n",
    "\n",
    "# Generate and plot\n",
    "x = data['EMPLOYEE_AGE'].values\n",
    "y = data['TECHM_EXPERIENCE'].values\n",
    "sb.kdeplot(x, y, cmap=cmap, shade=True, cut=5, ax=axes[0,0])\n",
    "axes[0,0].set( title = 'Age Vs TechMExperience')\n",
    "#display()\n",
    "cmap = sb.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True)\n",
    "\n",
    "# Generate and plot\n",
    "x = data['EMPLOYEE_AGE'].values\n",
    "y = data['NO_OF_LEAVES'].values\n",
    "sb.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,1])\n",
    "axes[0,1].set( title = 'Age Vs no_of_leaves')\n",
    "#display()\n",
    "\n",
    "cmap = sb.cubehelix_palette(start=0.666666666667, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = data['NO_OF_PREVIOUS_EMPLOYERS'].values\n",
    "y = data['EMPLOYEE_AGE'].values\n",
    "sb.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,2])\n",
    "axes[0,2].set( title = 'No_of_prev_employers Vs Age')\n",
    "display()\n",
    "'''\n",
    "cmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['DailyRate'].values\n",
    "y = attrition['DistanceFromHome'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,0])\n",
    "axes[1,0].set( title = 'Daily Rate against DistancefromHome')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=1.333333333333, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['DailyRate'].values\n",
    "y = attrition['JobSatisfaction'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,1])\n",
    "axes[1,1].set( title = 'Daily Rate against Job satisfaction')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['YearsAtCompany'].values\n",
    "y = attrition['JobSatisfaction'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,2])\n",
    "axes[1,2].set( title = 'Daily Rate against distance')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=2.0, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['YearsAtCompany'].values\n",
    "y = attrition['DailyRate'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,0])\n",
    "axes[2,0].set( title = 'Years at company against Daily Rate')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=2.333333333333, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['RelationshipSatisfaction'].values\n",
    "y = attrition['YearsWithCurrManager'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,1])\n",
    "axes[2,1].set( title = 'Relationship Satisfaction vs years with manager')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=2.666666666667, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['WorkLifeBalance'].values\n",
    "y = attrition['JobSatisfaction'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,2])\n",
    "axes[2,2].set( title = 'WorklifeBalance against Satisfaction')\n",
    "f.tight_layout()\n",
    "dispaly()\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_small = data# train.sample(frac=0.2).copy() # not small for now\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "sb.kdeplot(train_small.loc[data['BILLABILITY_STATUS'] == 'N', 'EMPLOYEE_AGE'], ax=ax[0], label='NoPay(0)')\n",
    "sb.kdeplot(train_small.loc[data['BILLABILITY_STATUS'] == 'Y', 'EMPLOYEE_AGE'], ax=ax[0], label='HasPay(1)')\n",
    "\n",
    "#\\train_small.loc[data['HasDetections'] == 0, 'DefaultBrowsersIdentifier'].hist(ax=ax[1])\n",
    "#train_small.loc[train['HasDetections'] == 1, 'DefaultBrowsersIdentifier'].hist(ax=ax[1])\n",
    "#ax[1].legend(['NoDetection(0)', 'HasDetection(1)'])\n",
    "\n",
    "plt.show()\n",
    "display()\n",
    "\"\"\"\n",
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'''\n",
    "# Segregating data set based on Attrition value\n",
    "attr_yes = data[data.TARGET_VALUE=='I'] #subset with Attrition = Yes\n",
    "attr_no = data[data.TARGET_VALUE=='A'] ##subset with Attrition = No\n",
    "'''\n",
    "attrition = data[(data['TARGET_VALUE'] == 'A' )]\n",
    "no_attrition = data[(data['TARGET_VALUE'] == 'I')]\n",
    "'''\n",
    "# Segregating data set based on Attrition value\n",
    "attr_yes = data[data.TARGET_VALUE=='I'] #subset with Attrition = Yes\n",
    "attr_no = data[data.TARGET_VALUE=='A'] ##subset with Attrition = No\n",
    "#Add columns to store discrete variables for Salary range and alcohol levels\n",
    "#Adding discrete valriables\n",
    "data['LeavesInterval'] = pd.cut(data['NO_OF_LEAVES'], 5, labels=['<9', '<15', '<24', '<32', '<33+'])\n",
    "data['TechMExpLvl'] = pd.cut(data['TECHM_EXPERIENCE'], 5, labels=['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5'])\n",
    "data['traveldate'] = pd.cut(data['LAST_ONSITE_TRAVEL_DAYS'], 5, labels=['0to3', '3to6', '6to9', '9to12', '12+'])\n",
    "\n",
    "\n",
    "\n",
    "#Factor Plot\n",
    "sb.factorplot(x =   'TARGET_VALUE',     # Categorical\n",
    "               y =   'NO_OF_LEAVES',          # Continuous\n",
    "               hue = 'DEPARTMENT_ID',   # Categorical\n",
    "               col = 'LeavesInterval',   # Categorical for graph columns\n",
    "               col_wrap=3,           # Wrap facet after two axes\n",
    "               kind = 'box',\n",
    "               data = data)\n",
    "plt.show()\n",
    "display()\n",
    "\n",
    "\n",
    "# Attrition count across Daily Rate, Years in company, Years since last promotion and level of stock options\n",
    "fig, ax2 = plt.subplots(2,2, figsize=(10,10))\n",
    "sb.countplot(x='LeavesInterval', data=attr_yes, ax = ax2[0,0])\n",
    "sb.countplot(x='TechMExpLvl', data=attr_yes, ax = ax2[0,1])\n",
    "sb.countplot(x='traveldate', data=attr_yes, ax = ax2[1,0])\n",
    "#sb.countplot(x='StockOptionLevel', data=attr_yes, ax = ax2[1,1])\n",
    "plt.show()\n",
    "display()\n",
    "'''\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#Count Plot\n",
    "# Attrition across Education level and corresponding percentage across the total number of employees\n",
    "total = data.shape[0] \n",
    "hrfig = sb.countplot(x='HIGHEST_EDUCATION', hue = 'TARGET_VALUE', data = data)\n",
    "#Above graph showcases the percentages of the employees across Education levels and corresponding attri\n",
    "for p in hrfig.patches:\n",
    "    height = p.get_height()\n",
    "    hrfig.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}'.format(height*100/total),\n",
    "            ha=\"center\") \n",
    "plt.show()\n",
    "display()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "dt = datetime.combine(date.today(), datetime.min.time())\n",
    "print(dt)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Attrition distribution bar plot\n",
    "\n",
    "plot = attrition_freq[['frequency_percent']].plot(kind=\"bar\");\n",
    "plot.set_title(\"Attrition Distribution\", fontsize=40);\n",
    "plot.grid(color='lightgray', alpha=0.5);\n",
    "display()\n",
    "\"\"\"\n",
    "\"\"\"Just by a quick inspection of the counts of the number of 'Yes' and 'No' in the target variable tells us that there is quite a large skew in target variable.\n",
    "Therefore we have to keep in mind that there is quite a big imbalance in our target variable. Many statistical techniques have been put forth to treat imbalances in data (oversampling or undersampling).\n",
    "In this notebook, I will use an oversampling technique known as SMOTE to treat this imbalance.\n",
    "We will see the prediction model with and without SMOTE treatment for imbalance class issue.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Step 4 - Feature Engineering\n",
    "Feature engineering is one aspect which provided a huge impact on the outcome rather than the model. Here, we try at creating new features with the existing variables we have based on my assumptions.\n",
    "\n",
    "Step 4.1 - Addition of New Features\n",
    "Tenure per job: Usually, people who have worked with many companies but for small periods at every organization tend to leave early as they always need a change of Organization to keep them going.\n",
    "Years without Change: For any person, a change either in role or job level or responsibility is needed to keep the work exciting to continue. We create a variable to see how many years it has been for an employee without any sort of change using Promotion, Role and Job Change as a metric to cover different variants of change.\n",
    "Compensation Ratio: Compa Ratio is the ratio of the actual pay of an Employee to the midpoint of a salary range. The salary range can be that of his/her department or organization or role. The benchmark numbers can be a organizations pay or Industry average.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "emp_proc_df = data.copy() # Copy cleaned dataset for feature engineering\n",
    "\n",
    "emp_proc_df['TenurePerJob'] = 0\n",
    "\n",
    "for i in range(0, len(emp_proc_df)):\n",
    "    if emp_proc_df.loc[i,'NO_OF_PREVIOUS_EMPLOYERS'] > 0:\n",
    "        emp_proc_df.loc[i,'TenurePerJob'] = emp_proc_df.loc[i,'TOTAL_EXPERIENCE'] / emp_proc_df.loc[i,'NO_OF_PREVIOUS_EMPLOYERS']\n",
    "\"\"\"\n",
    "emp_proc_df['YearWithoutChange1'] = emp_proc_df['YearsInCurrentRole'] - emp_proc_df['YearsSinceLastPromotion']\n",
    "emp_proc_df['YearWithoutChange2'] = emp_proc_df['TotalWorkingYears'] - emp_proc_df['YearsSinceLastPromotion']\n",
    "\n",
    "monthly_income_median = np.median(emp_proc_df['MonthlyIncome'])\n",
    "emp_proc_df['CompRatioOverall'] = emp_proc_df['MonthlyIncome'] / monthly_income_median\n",
    "\n",
    "print('Dataset dimension: {} rows, {} columns'.format(emp_proc_df.shape[0], emp_proc_df.shape[1]))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_col_names = emp_proc_df.columns.tolist()\n",
    "num_col_names = emp_proc_df.select_dtypes(include=[np.int64,np.float64]).columns.tolist() # Get numerical feature names\n",
    "\n",
    "# Preparing list of ordered categorical features\n",
    "for i in emp_proc_df:\n",
    "    if emp_proc_df[i].dtype==object:\n",
    "        num_cat_col_names = emp_proc_df[i]\n",
    "#num_cat_col_names = ['DEPARTMENT_ID', 'TARGET_VALUE', 'OFFICERCODE_DESC', 'ALL_PREVIOUS_RAINGS', 'JobSatisfaction',\n",
    "                     #'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance', 'StockOptionLevel']\n",
    "\n",
    "target = ['TARGET_VALUE']\n",
    "\n",
    "num_col_names = list(set(num_col_names) - set(num_cat_col_names)) # Numerical features w/o Ordered Categorical features\n",
    "cat_col_names = list(set(full_col_names) - set(num_col_names) - set(target)) # Categorical & Ordered Categorical features\n",
    "\n",
    "print('Total number of numerical features: ', len(num_col_names))\n",
    "print('Total number of categorical & ordered categorical features: ', len(cat_col_names))\n",
    "cat_emp_df = emp_proc_df[cat_col_names]\n",
    "num_emp_df = emp_proc_df[num_col_names]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Transform Numerical Features\n",
    "#In order to fix the skewness, lets take the log for all numerical features with an absolute skew greater than 80% (Note: log+1, to avoid division by zero issues).\n",
    "\n",
    "# Let's create dummy variables for each categorical attribute for training our calssification model\n",
    "for col in num_col_names:\n",
    "    if num_emp_df[col].skew() > 0.80:\n",
    "        num_emp_df[col] = np.log1p(num_emp_df[col])\n",
    "\n",
    "num_emp_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" #Transform Categorical Features\n",
    "Machine Learning model works only on numerical datasets, hence, we need to transform categorical features into numerical features.\n",
    "One of the best strategy is to convert each category value into a new column and assigns 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set.\n",
    "This approach is also called as \"One Hot Encoding\". We can use Pandas feature get_dummies to achieve this transformation.\n",
    "There is another way to handled ordered categorical feature is to give ordered value based on their definitions i.e Low-Meidum-High would be 1-2-3. We can try this approach some other time. But, this can be evaluated to check the performance of the model.\n",
    "\"\"\"\n",
    "# Let's create dummy variables for each categorical attribute for training our calssification model\n",
    "for col in cat_col_names:\n",
    "    col_dummies = pd.get_dummies(cat_emp_df[col], prefix=col)\n",
    "    cat_emp_df = pd.concat([cat_emp_df, col_dummies], axis=1)\n",
    "\n",
    "# Use the pandas apply method to numerically encode our attrition target variable\n",
    "attrition_target = emp_proc_df['TARGET_VALUE'].map({'I':1, 'A':0})\n",
    "\n",
    "# Drop categorical feature for which dummy variables have been created\n",
    "\n",
    "cat_emp_df.drop(cat_col_names, axis=1, inplace=True)\n",
    "cat_emp_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Correlation of Numerical Features against Attrition\n",
    "num_corr_df = num_emp_df[['EMPLOYEE_AGE', 'NO_OF_MANAGER_REPORTEES', 'TOTAL_REPORTEES']]\n",
    "corr_df = pd.concat([num_corr_df, attrition_target], axis=1)\n",
    "corr = corr_df.corr()\n",
    "display()\n",
    "plt.figure(figsize = (10, 8))\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sb.axes_style(\"white\")\n",
    "#sns.heatmap(data=corr, annot=True, mask=mask, square=True, linewidths=.5, vmin=-1, vmax=1, cmap=\"YlGnBu\")\n",
    "sb.heatmap(data=corr, annot=True, square=True, linewidths=.5, vmin=-1, vmax=1, cmap=\"YlGnBu\")\n",
    "plt.show()\n",
    "display()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Another way to check for correlation between attributes is to use Pandas scatter_matrix function,\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(corr_df, figsize=(16, 10));\n",
    "display()\n",
    "\n",
    "\n",
    "\n",
    "# Concat the two dataframes together columnwise\n",
    "final_emp_df = pd.concat([num_emp_df, cat_emp_df], axis=1)\n",
    "\n",
    "print('Dataset dimension after treating categorical features with dummy variables: {} rows, {} columns'.format(final_emp_df.shape[0], final_emp_df.shape[1]))\n",
    "final_emp_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#Model Building and Validation\n",
    "Since, we have to predict a binary class, we will be using classification models for training & predicting Employee Attrition. We need to keep in mind that our focus should be to have a better accuracy of predicting attrition i.e. Attrition = 1 which in confusion matrix will be \"True Positive\". However, we should not forget the prediction accuracy of not qualifying for attrition i.e. Attrition = 0 which will be \"True Negative\" in confusion matrix.\n",
    "\n",
    "So, we need to focus on four parameters:\n",
    "\n",
    "Accuracy: Overall, how often is the classifier correct? i.e {(TP+TN)/Total}\n",
    "True Positive Rate: When it's actually yes, how often does it predict yes? default_ind = 1, {TP/Actual YES}, this is also known as \"Sensitivity\" or \"Recall\"\n",
    "Precision: When it predicts yes, how often is it correct? i.e. {TP/(TP+FP)}\n",
    "Specificity: When it's actually no, how often does it predict no? default_ind = 0, {TN/actual NO}\n",
    "Cross Validation Score: Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. Later, you test your model on this sample before finalizing it. Do this for k folds and take mean of accuracy scores of the k fold models.\n",
    "F1 Score: This is a weighted average of the true positive rate (recall) and precision.\n",
    "ROC Curve: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class.\n",
    "Above information was taken and more details can be found at https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "\n",
    "Step 5.1 - Prepare Train & Test Dataset\n",
    "The data should be divided into train and test data. We will use train_test_split feature to divide the data and we will be using 70-30 ratio\n",
    "\"\"\"\n",
    "# Import the train_test_split method\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Split data into train and test sets as well as for validation and testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(final_emp_df, attrition_target,\n",
    "                                                  test_size= 0.30, random_state=42);\n",
    "\n",
    "print(\"Stratified Sampling: \", len(X_train), \"train set +\", len(X_val), \"validation set\")\n",
    "\n",
    "# Stratified Splitting\n",
    "#split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "#for train_index, test_index in split.split(emp_data_proc, emp_data_proc['Gender']):\n",
    "#    strat_train_set = emp_data_proc.loc[train_index]\n",
    "#    strat_test_set = emp_data_proc.loc[test_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "def gen_model_performance(actual_target, pred_target):\n",
    "    model_conf_matrix = confusion_matrix(actual_target, pred_target)\n",
    "    model_roc_score = roc_auc_score(actual_target, pred_target)\n",
    "    model_accuracy = accuracy_score(actual_target, pred_target) * 100.0\n",
    "    \n",
    "    TP = model_conf_matrix[1][1]; TN = model_conf_matrix[0][0]; \n",
    "    FP = model_conf_matrix[0][1]; FN = model_conf_matrix[1][0];\n",
    "    sensitivity = TP / float(TP + FN) * 100.0; specificity = TN / float(TN + FP) * 100.0;\n",
    "    precision = TP / float(TP + FP) * 100.0;\n",
    "    \n",
    "    return sensitivity, specificity, model_accuracy, precision, model_roc_score\n",
    "def evaluate_model_score(X, y, scoring='accuracy'):\n",
    "    \n",
    "    logreg_model = LogisticRegression(random_state=0)\n",
    "    logreg_cv_model = LogisticRegressionCV()\n",
    "    rfc_model = RandomForestClassifier()\n",
    "    extrees_model = ExtraTreesClassifier()\n",
    "    gboost_model = GradientBoostingClassifier()\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    aboost_model = AdaBoostClassifier()\n",
    "    gnb_model = GaussianNB()\n",
    "\n",
    "    models = [logreg_model, logreg_cv_model, dt_model, rfc_model, \n",
    "              extrees_model, gboost_model, aboost_model, gnb_model]\n",
    "    \n",
    "    model_results = pd.DataFrame(columns = [\"Model\", \"Accuracy\", \"Precision\", \"CV Score\",\n",
    "                                            \"Sensitivity\",\"Specificity\",\"ROC Score\"])\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X, y,)\n",
    "        y_pred = model.predict(X)\n",
    "        score = cross_val_score(model, X, y, cv=5, scoring=scoring)\n",
    "        \n",
    "        sensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y, y_pred)\n",
    "    \n",
    "        scores = cross_val_score(model, X, y, cv=5)\n",
    "    \n",
    "        model_results = model_results.append({\"Model\": model.__class__.__name__,\n",
    "                              \"Accuracy\": accuracy, \"Precision\": precision,\n",
    "                              \"CV Score\": scores.mean()*100.0,\n",
    "                              \"Sensitivity\": sensitivity, \"Specificity\": specificity,\n",
    "                              \"ROC Score\": roc_score}, ignore_index=True)\n",
    "    return model_results\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist=[fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  i.CURRENT_RATING.fillna(i.LAST_RATING,inplace=True)\n",
    "  i.CURRENT_RATING.fillna(i.ALL_PREVIOUS_RAINGS.str[-1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(fd1[fd1.CURRENT_RATING.isnull()].ALL_PREVIOUS_RAING.shape[0]):\n",
    "  a=fd1[fd1.CURRENT_RATING.isnull()].ALL_PREVIOUS_RAING[i]\n",
    "  print(a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist=[fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  i.PRIMARY_SKILL.fillna(i.SECONDARY_SKILL,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''s='20170601'\n",
    "date=pd.to_datetime(s)\n",
    "date'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_df:\n",
    "  i.replace(' ', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da1=pd.to_datetime('20160301')\n",
    "da2=pd.to_datetime('20160601')\n",
    "fd1['PROMOTION_FLAG']=fd1.apply(lambda fd1 : 1 if fd1.LAST_PROMOTION_DATE.date()>=da1.date() and fd1.LAST_PROMOTION_DATE.date()<=da2.date()  else 0,axis=1)\n",
    "da1=pd.to_datetime('20160601')\n",
    "da2=pd.to_datetime('20160901')\n",
    "fd2['PROMOTION_FLAG']=fd2.apply(lambda fd2 : 1 if fd2.LAST_PROMOTION_DATE.date()>da1.date() and fd2.LAST_PROMOTION_DATE.date()<da2.date() else 0,axis=1)\n",
    "da1=pd.to_datetime('20160901')\n",
    "da2=pd.to_datetime('20161201')\n",
    "fd3['PROMOTION_FLAG']=fd3.apply(lambda fd3 : 1 if fd3.LAST_PROMOTION_DATE.date()>da1.date() and fd3.LAST_PROMOTION_DATE.date()<da2.date() else 0,axis=1)\n",
    "da1=pd.to_datetime('20161201')\n",
    "da2=pd.to_datetime('20170301')\n",
    "fd4['PROMOTION_FLAG']=fd4.apply(lambda fd4 : 1 if fd4.LAST_PROMOTION_DATE.date()>da1.date() and fd4.LAST_PROMOTION_DATE.date()<da2.date() else 0,axis=1)\n",
    "da1=pd.to_datetime('20170301')\n",
    "da2=pd.to_datetime('20170601')\n",
    "fd5['PROMOTION_FLAG']=fd5.apply(lambda fd5 : 1 if fd5.LAST_PROMOTION_DATE.date()>da1.date() and fd5.LAST_PROMOTION_DATE.date()<da2.date() else 0,axis=1)\n",
    "da1=pd.to_datetime('20170601')\n",
    "da2=pd.to_datetime('20170901')\n",
    "fd6['PROMOTION_FLAG']=fd6.apply(lambda fd6 : 1 if fd6.LAST_PROMOTION_DATE.date()>da1.date() and fd6.LAST_PROMOTION_DATE.date()<da2.date() else 0,axis=1)\n",
    "da1=pd.to_datetime('20170901')\n",
    "da2=pd.to_datetime('20171201')\n",
    "fd7['PROMOTION_FLAG']=fd7.apply(lambda fd7 : 1 if fd7.LAST_PROMOTION_DATE.date()>da1.date() and fd7.LAST_PROMOTION_DATE.date()<da2.date() else 0,axis=1)\n",
    "da1=pd.to_datetime('20171201')\n",
    "da2=pd.to_datetime('20180301')\n",
    "fd8['PROMOTION_FLAG']=fd8.apply(lambda fd8 : 1 if fd8.LAST_PROMOTION_DATE.date()>da1.date() and fd8.LAST_PROMOTION_DATE.date()<da2.date() else 0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da=datetime(2016,1,1)\n",
    "# fd1.LAST_PROMOTION_DATE.dt.date<da.date()\n",
    "# fd1.LAST_PROMOTION_DATE.dt.date\n",
    "# # fd1.LAST_PROMOTION_DATE.dtypespd\n",
    "# pd.to_datetime(da)\n",
    "# # da.date()\n",
    "# np.datetime64(da)\n",
    "da.date().month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd1.LAST_PROMOTION_DATE.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "attrition_notebook",
  "notebookId": 2315848079239377
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
