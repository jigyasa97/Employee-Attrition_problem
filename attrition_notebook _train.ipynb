{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%fs rm -r /FileStore/tables\n",
    "##import os\n",
    "#os.remove('/dbfs/FileStore/tables/PRED_30JUN17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#import libraries\n",
    "#from pyspark.sql.functions import avg\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams['font.size'] = 22\n",
    "import warnings    # To suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/dbfs/FileStore/tables/PRED_31MAR17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df2 = pd.read_csv('/dbfs/FileStore/tables/PRED_30JUN17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df3 = pd.read_csv('/dbfs/FileStore/tables/PRED_31OCT17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df4 = pd.read_csv('/dbfs/FileStore/tables/PRED_31DEC17.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df5 = pd.read_csv('/dbfs/FileStore/tables/PRED_31MAR18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df6 = pd.read_csv('/dbfs/FileStore/tables/PRED_30JUN18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df7 = pd.read_csv('/dbfs/FileStore/tables/PRED_31OCT18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df8 = pd.read_csv('/dbfs/FileStore/tables/PRED_31DEC18.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "df9 = pd.read_csv('/dbfs/FileStore/tables/PRED_MARR19.csv', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "\n",
    "\n",
    "list_df = [df1,df2,df3,df4,df5,df6,df7,df8,df9]\n",
    "for i in list_df:\n",
    "  i = i.drop_duplicates().reset_index(drop = True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[3]: &#34;\\nfor i in list_df:\\n  i.replace(&#39; &#39;, np.nan, inplace=True)\\n  &#34;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "for i in list_df:\n",
    "  i.replace(' ', np.nan, inplace=True)\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp1=df1[df1.EMP_STATUS=='A']\n",
    "temp2=df2[df2.EMP_STATUS=='A']\n",
    "temp3=df3[df3.EMP_STATUS=='A']\n",
    "temp4=df4[df4.EMP_STATUS=='A']\n",
    "temp5=df5[df5.EMP_STATUS=='A']\n",
    "temp6=df6[df6.EMP_STATUS=='A']\n",
    "temp7=df7[df7.EMP_STATUS=='A']\n",
    "temp8=df8[df8.EMP_STATUS=='A']\n",
    "temp9=df9[df9.EMP_STATUS=='A']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd1=pd.merge(temp1[['MASK_EMPLOYEEID','EMP_STATUS']],df2,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd2=pd.merge(temp2[['MASK_EMPLOYEEID','EMP_STATUS']],df3,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd3=pd.merge(temp3[['MASK_EMPLOYEEID','EMP_STATUS']],df4,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd4=pd.merge(temp4[['MASK_EMPLOYEEID','EMP_STATUS']],df5,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd5=pd.merge(temp5[['MASK_EMPLOYEEID','EMP_STATUS']],df6,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd6=pd.merge(temp6[['MASK_EMPLOYEEID','EMP_STATUS']],df7,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd7=pd.merge(temp7[['MASK_EMPLOYEEID','EMP_STATUS']],df8,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "fd8=pd.merge(temp8[['MASK_EMPLOYEEID','EMP_STATUS']],df9,how='inner',on=['MASK_EMPLOYEEID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Deletng EMP_STATUS column from original df\n",
    "for i in range(8):\n",
    "    data=globals()[\"fd\" + str(i + 1)]\n",
    "    data.drop(['EMP_STATUS_x'],axis=1,inplace=True)\n",
    "    data.rename(columns={'EMP_STATUS_y':'TARGET_VALUE'},inplace=True)\n",
    "    globals()[\"fd\" + str(i + 1)]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_set = pd.concat([fd1,fd2,fd3,fd4,fd5,fd6]).drop_duplicates().reset_index(drop=True)\n",
    "validation_set = pd.concat([fd7,fd8]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MASK_EMPLOYEEID</th>\n",
       "      <th>EMPLOYEE_AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NO_OF_LEAVES</th>\n",
       "      <th>TOTAL_REPORTEES</th>\n",
       "      <th>BAND</th>\n",
       "      <th>DEPARTMENT_ID</th>\n",
       "      <th>NO_OF_MANAGER_REPORTEES</th>\n",
       "      <th>TECHM_EXPERIENCE</th>\n",
       "      <th>TOTAL_EXPERIENCE</th>\n",
       "      <th>LAST_HIKE_PERCENT</th>\n",
       "      <th>MANAGER_RATING</th>\n",
       "      <th>NO_OF_PREVIOUS_EMPLOYERS</th>\n",
       "      <th>TENURE_LAST_EMPLOPYER</th>\n",
       "      <th>HIGHEST_EDUCATION</th>\n",
       "      <th>QUARTILE</th>\n",
       "      <th>LAST_PROMOTION_DATE</th>\n",
       "      <th>TOTAL_PROMOTIONS</th>\n",
       "      <th>MANAGER_LAST_PROMOTION</th>\n",
       "      <th>LAST_ONSITE_TRAVEL_DATE</th>\n",
       "      <th>LAST_ONSITE_TRAVEL_DAYS</th>\n",
       "      <th>REWARDS</th>\n",
       "      <th>PRIMARY_SKILL</th>\n",
       "      <th>SECONDARY_SKILL</th>\n",
       "      <th>BASE_LOCATION_CITY</th>\n",
       "      <th>CURRENT_LOCATION_CITY</th>\n",
       "      <th>HIRE_LOCATION_CITY</th>\n",
       "      <th>RESIGNATION_FLG</th>\n",
       "      <th>BILLABILITY_STATUS</th>\n",
       "      <th>CURRENT_RATING</th>\n",
       "      <th>LAST_RATING</th>\n",
       "      <th>ALL_PREVIOUS_RAINGS</th>\n",
       "      <th>LATEST_HIKE_PERCENT</th>\n",
       "      <th>OFFICERCODE_DESC</th>\n",
       "      <th>GRIEVANCE_FLG</th>\n",
       "      <th>TARGET_VALUE</th>\n",
       "      <th>HIRE_EXIT_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E006B574A10D5213</td>\n",
       "      <td>34.0</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>P1</td>\n",
       "      <td>IBU-CTP-01</td>\n",
       "      <td>22</td>\n",
       "      <td>5.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4.92</td>\n",
       "      <td>BE/BTech/BScTech</td>\n",
       "      <td>3</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Business Analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BENGALURU</td>\n",
       "      <td>BENGALURU</td>\n",
       "      <td>BENGALURU</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>2013-E,2014-C,2015-C,2016-C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lateral Joinees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>2012-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE72B59D51C7B2AE</td>\n",
       "      <td>36.0</td>\n",
       "      <td>F</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>P1</td>\n",
       "      <td>IASE</td>\n",
       "      <td>74</td>\n",
       "      <td>12.3</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>DIPLOMA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Software Quality Assurance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-C,2007-E,2008-E,2009-E,2010-E,2011-C,2012...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Initial Training Program</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2005-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0776ED5162671E1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>P2</td>\n",
       "      <td>IASE</td>\n",
       "      <td>61</td>\n",
       "      <td>11.8</td>\n",
       "      <td>22.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2013-12-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Project Management</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-C,2007-C,2008-X,2009-C,2010-X,2011-C,2012...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lateral Joinees</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2005-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5654CB68AE69AAD9</td>\n",
       "      <td>47.0</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>P2</td>\n",
       "      <td>IBU-ANZ3</td>\n",
       "      <td>30</td>\n",
       "      <td>10.4</td>\n",
       "      <td>20.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>8.44</td>\n",
       "      <td>Company Secretery</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-02-21</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "      <td>IBM-DB2-Performance Expert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-C,2009-C,2010-X,2011-C,2012-X,2013-C,2014...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FCTP</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2007-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2E8CB900573C1337</td>\n",
       "      <td>40.0</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>U4</td>\n",
       "      <td>IMS04C</td>\n",
       "      <td>34</td>\n",
       "      <td>10.1</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Application Design and Arch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-E,2009-E,2010-C,2011-C,2012-E,2013-E,2014...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lateral Joinees</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2007-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = df2.append(df3)\n",
    "f1 = df2.append(df4)\n",
    "triple1=pd.merge(temp1[['MASK_EMPLOYEEID','EMP_STATUS']],f1,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "f2 = df3.append(df4)\n",
    "f2 = df3.append(df5)\n",
    "triple2=pd.merge(temp2[['MASK_EMPLOYEEID','EMP_STATUS']],f2,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "f3 = df4.append(df5)\n",
    "f3 = df4.append(df6)\n",
    "triple3=pd.merge(temp3[['MASK_EMPLOYEEID','EMP_STATUS']],f3,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "f4 = df5.append(df6)\n",
    "f4 = df5.append(df7)\n",
    "triple4=pd.merge(temp4[['MASK_EMPLOYEEID','EMP_STATUS']],f4,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "\n",
    "f5 = df6.append(df7)\n",
    "f5 = df6.append(df8)\n",
    "triple5=pd.merge(temp5[['MASK_EMPLOYEEID','EMP_STATUS']],f5,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "\n",
    "\n",
    "list_f = [triple1,triple2,triple3,triple4,triple5]\n",
    "for i in list_f:\n",
    "  i['MASK_EMPLOYEEID'] = i['MASK_EMPLOYEEID'].drop_duplicates( keep='last', inplace=False)\n",
    "\n",
    "\n",
    "#triples['MASK_EMPLOYEEID'].drop_duplicates().reset_index(drop = True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Deletng EMP_STATUS column from original df\n",
    "for i in range(5):\n",
    "    data=globals()[\"triple\" + str(i + 1)]\n",
    "    data.drop(['EMP_STATUS_x'],axis=1,inplace=True)\n",
    "    data.rename(columns={'EMP_STATUS_y':'TARGET_VALUE'},inplace=True)\n",
    "    globals()[\"triple\" + str(i + 1)]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[11]: &#34;y1 = df2.append(df3)\\ny1 = df2.append(df4)\\nyear1=pd.merge(temp1[[&#39;MASK_EMPLOYEEID&#39;,&#39;EMP_STATUS&#39;]],y1,how=&#39;inner&#39;,on=[&#39;MASK_EMPLOYEEID&#39;])\\nyear1[&#39;MASK_EMPLOYEEID&#39;] = year1[&#39;MASK_EMPLOYEEID&#39;].drop_duplicates( keep=&#39;last&#39;, inplace=False)\\n#year1 = year1.drop_duplicates(keep=False,inplace=True)\\n\\ny2 = df6.append(df7)\\ny2 = df6.append(df8)\\nyear2=pd.merge(temp5[[&#39;MASK_EMPLOYEEID&#39;,&#39;EMP_STATUS&#39;]],y2,how=&#39;inner&#39;,on=[&#39;MASK_EMPLOYEEID&#39;])\\nyear2[&#39;MASK_EMPLOYEEID&#39;] = year2[&#39;MASK_EMPLOYEEID&#39;].drop_duplicates( keep=&#39;last&#39;, inplace=False)\\n#year2 = year2.drop_duplicates(keep=False,inplace=True)\\n\\n&#34;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''y1 = df2.append(df3)\n",
    "y1 = df2.append(df4)\n",
    "year1=pd.merge(temp1[['MASK_EMPLOYEEID','EMP_STATUS']],y1,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "year1['MASK_EMPLOYEEID'] = year1['MASK_EMPLOYEEID'].drop_duplicates( keep='last', inplace=False)\n",
    "#year1 = year1.drop_duplicates(keep=False,inplace=True)\n",
    "\n",
    "y2 = df6.append(df7)\n",
    "y2 = df6.append(df8)\n",
    "year2=pd.merge(temp5[['MASK_EMPLOYEEID','EMP_STATUS']],y2,how='inner',on=['MASK_EMPLOYEEID'])\n",
    "year2['MASK_EMPLOYEEID'] = year2['MASK_EMPLOYEEID'].drop_duplicates( keep='last', inplace=False)\n",
    "#year2 = year2.drop_duplicates(keep=False,inplace=True)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Deletng EMP_STATUS column from original df\n",
    "for i in range(2):\n",
    "    data=globals()[\"year\" + str(i + 1)]\n",
    "    data.drop(['EMP_STATUS_x'],axis=1,inplace=True)\n",
    "    data.rename(columns={'EMP_STATUS_y':'TARGET_VALUE'},inplace=True)\n",
    "    globals()[\"year\" + str(i + 1)]=data'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "# print(fd1.QUARTILE.value_counts())\n",
    "for i in llist:\n",
    "  print('------------------')\n",
    "  print(i.QUARTILE.value_counts())\n",
    "  print((i[i.TARGET_VALUE=='I'].QUARTILE.value_counts()*100)/i.QUARTILE.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=fd1.QUARTILE.value_counts().index\n",
    "# colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\"]\n",
    "#explode=[0,0,0,0,0,0]\n",
    "sizes=(fd1[fd1.TARGET_VALUE=='I'].QUARTILE.value_counts()*100)/fd1.QUARTILE.value_counts().values\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\")\n",
    "plt.title(\"Quartile Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=fd1.QUARTILE.value_counts().index\n",
    "# colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\"]\n",
    "#explode=[0,0,0,0,0,0]\n",
    "sizes=i[i.TOTAL_EXPERIENCE<=3].QUARTILE.value_counts().values\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\")\n",
    "plt.title(\"Education Field Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1=pd.crosstab(data[\"QUARTILE\"],data['TARGET_VALUE'])\n",
    "var1.div(var1.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize = (10,5)) \n",
    "  #plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total percentage of experienced and inexperienced employee in each quartile for each quarter\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "l=[0.0,1.0,2.0,3.0,4.0]\n",
    "j=1\n",
    "for i in llist:\n",
    "  print('Quarter ' , str(j))\n",
    "  print('total percent of inexperienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE<=3].QUARTILE.value_counts()*100/i.QUARTILE.value_counts())\n",
    "  print('total percent of experienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE>3].QUARTILE.value_counts()*100/i.QUARTILE.value_counts())\n",
    "  j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of experienced and inexperienced people who are attriting\n",
    "\n",
    "\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "l=[0.0,1.0,2.0,3.0,4.0]\n",
    "j=1\n",
    "for i in llist:\n",
    "  print('Quarter ' , str(j))\n",
    "  print('total percent of inexperienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE<=3][i.TARGET_VALUE=='I'].QUARTILE.value_counts()*100/i[i.TOTAL_EXPERIENCE<=3].QUARTILE.value_counts())\n",
    "  print('total percent of experienced employee')\n",
    "  print(i[i.TOTAL_EXPERIENCE>3][i.TARGET_VALUE=='I'].QUARTILE.value_counts()*100/i[i.TOTAL_EXPERIENCE>3].QUARTILE.value_counts())\n",
    "  print('-----------')\n",
    "  j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tech mahindra experience per quartile\n",
    "\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "l=[0.0,1.0,2.0,3.0,4.0]\n",
    "j=1\n",
    "for i in llist:\n",
    "  print(i[i.TECHM_EXPERIENCE<=2].QUARTILE.value_counts()*100/i.QUARTILE.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (fd1[fd1.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts()>3).sum(),fd1[fd1.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts()\n",
    "# fd1[fd1.TARGET_VALUE=='I'][fd1.TOTAL_EXPERIENCE>2].TOTAL_PROMOTIONS.value_counts(),fd1[fd1.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.mean()\n",
    "\n",
    "j=1\n",
    "for i in llist:\n",
    "    print('QUARTER NUMBER ',str(j))\n",
    "    print('the percentage of total inexperienced and experienced',i[i.TOTAL_EXPERIENCE<=2].TOTAL_EXPERIENCE.value_counts().sum()*100/i.TOTAL_EXPERIENCE.value_counts().sum(),i[i.TOTAL_EXPERIENCE>2].TOTAL_EXPERIENCE.value_counts().sum()*100/i.TOTAL_EXPERIENCE.value_counts().sum())\n",
    "    print('percent of the inexperienced who are leaving',i[i.TOTAL_EXPERIENCE<=3][i.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts().sum()*100/i[i.TOTAL_EXPERIENCE<=3].TOTAL_EXPERIENCE.value_counts().sum())\n",
    "    print('percent of the experienced who are leaving',i[i.TOTAL_EXPERIENCE<=3][i.TARGET_VALUE=='I'].TOTAL_EXPERIENCE.value_counts().sum()*100/i[i.TOTAL_EXPERIENCE>3].TOTAL_EXPERIENCE.value_counts().sum())\n",
    "    print('-------------------')\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j=1\n",
    "for i in llist:\n",
    "  print('for QUARTER ',str(j))\n",
    "  print((i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE<=3].TOTAL_PROMOTIONS.value_counts()*100)/i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE<=3].TOTAL_PROMOTIONS.value_counts().sum())\n",
    "  print((i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE>3].TOTAL_PROMOTIONS.value_counts()*100)/i[i.TARGET_VALUE=='I'][i.TOTAL_EXPERIENCE>3].TOTAL_PROMOTIONS.value_counts().sum())\n",
    "  print('-------------------------')\n",
    "  j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripplott(variable):\n",
    "  f,ax=plt.subplots(figsize=(18,18))\n",
    "  sb.set(rc={'figure.figsize':(16,20)})\n",
    "  sb.stripplot(x='TARGET_VALUE', y=variable, data=fd1, alpha=0.3, jitter=True);\n",
    "  # plt.plot()\n",
    "  display()\n",
    "  stripplott(\"TOTAL_EXPERIENCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(figsize=(18,18))\n",
    "sb.set(rc={'figure.figsize':(100,50)})\n",
    "# sb.countplot(x='BAND',data=fd3)\n",
    "sb.factorplot(x='TARGET_VALUE',col='TOTAL_PROMOTIONS',kind='count',data=fd1)\n",
    "# fd1.groupby('NO_OF_LEAVES').BAND.hist(alpha=0.6)\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(300, 100))\n",
    "# sb.kdeplot(fd1.loc[fd1['TARGET_VALUE'] == 'I', 'NO_OF_MANAGER_REPORTEES'], ax=ax[0], label='INACTIVE(0)')\n",
    "# sb.kdeplot(fd1.loc[fd1['TARGET_VALUE'] == 'A', 'NO_OF_MANAGER_REPORTEES'], ax=ax[0], label='ACTIVE(1)')\n",
    "# plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain original copy\n",
    "\n",
    "data = fd8[fd8.TARGET_VALUE=='I'].copy()\n",
    "data1 = training_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare, chi2_contingency, chi2\n",
    "\n",
    "# One-way chisquare test to see if states are evenly distributed\n",
    "chisquare(df['GRIEVANCE_FLG'].value_counts())\n",
    "\n",
    "#Contingency Table\n",
    "contingency_table=pd.crosstab(df[\"HIGHEST_EDUCATION\"],df[\"TARGET_VALUE\"])\n",
    "print('contingency_table :-\\n',contingency_table)\n",
    "\n",
    "#Observed Values\n",
    "Observed_Values = contingency_table.values \n",
    "print(\"Observed Values :-\\n\",Observed_Values)\n",
    "\n",
    "#Expected Values\n",
    "import scipy.stats\n",
    "b=scipy.stats.chi2_contingency(contingency_table)\n",
    "Expected_Values = b[3]\n",
    "print(\"Expected Values :-\\n\",Expected_Values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Degree of Freedom\n",
    "no_of_rows=len(contingency_table.iloc[0:2,0])\n",
    "no_of_columns=len(contingency_table.iloc[0,0:2])\n",
    "df=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",df)\n",
    "\n",
    "#or\n",
    "#df=b[2]\n",
    "#print(\"Degree of Freedom:-\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Significance Level 5%\n",
    "alpha=0.05\n",
    "#chi-square statistic - Ï‡2\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_Values,Expected_Values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "#critical_value\n",
    "critical_value=chi2.ppf(q=1-alpha,df=df)\n",
    "print('critical_value:',critical_value)\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=df)\n",
    "print('p-value:',p_value)\n",
    "#p-value: 0.7641771556220945\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',df)\n",
    "print('chi-square statistic:',chi_square_statistic)\n",
    "print('critical_value:',critical_value)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare chi_square_statistic with critical_value and p-value which is the probability of getting chi-square>0.09 (chi_square_statistic)\n",
    "if chi_square_statistic>=critical_value:\n",
    "    print(\"Reject H0,There is a relationship between 2 categorical variables\")\n",
    "else:\n",
    "    print(\"Retain H0,There is no relationship between 2 categorical variables\")\n",
    "    \n",
    "if p_value<=alpha:\n",
    "    print(\"Reject H0,There is a relationship between 2 categorical variables\")\n",
    "else:\n",
    "    print(\"Retain H0,There is no relationship between 2 categorical variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = data1.select_dtypes(include=['object']).copy()\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fd1['PROMOTION_FLAG']=fd1.apply(lambda fd1 : 1 if fd1.LAST_PROMOTION_DATE.year>2016 and fd1.LAST_PROMOTION_DATE.month<6 else 0,axis=1)\n",
    "data1['PROMOTION_FLAG']=data1.apply(lambda data1 : 1 if data1.LAST_PROMOTION_DATE.year>2016 and data1.LAST_PROMOTION_DATE.month<6 else 0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2=fd1.copy()\n",
    "#data2.info()\n",
    "data1['CITY_FLAG']=data1.apply(lambda data1 : 1 if data1.HIRE_LOCATION_CITY==data1.CURRENT_LOCATION_CITY else 0,axis=1 )\n",
    "data1.CITY_FLAG.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.PRIMARY_SKILL.fillna(data1.SECONDARY_SKILL.str[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.CURRENT_RATING.fillna(data1.LAST_RATING,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.CURRENT_RATING.fillna(data1.ALL_PREVIOUS_RAINGS.str[-1],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"band.pickle\",\"rb\")\n",
    "dict_band = pickle.load(pickle_in)\n",
    "print(dict_band)\n",
    "\n",
    "pickle_in = open(\"education.pickle\",\"rb\")\n",
    "dict_education = pickle.load(pickle_in)\n",
    "print(dict_education)\n",
    "\n",
    "pickle_in = open(\"rating.pickle\",\"rb\")\n",
    "dict_rating = pickle.load(pickle_in)\n",
    "print(dict_rating)\n",
    "\n",
    "pickle_in = open(\"education_name.pickle\",\"rb\")\n",
    "dict_edu_name = pickle.load(pickle_in)\n",
    "print(dict_edu_name)\n",
    "#print(example_dict[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['HIGHEST_EDUCATION'].replace(dict_edu_name, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['BAND'].replace(dict_band, inplace=True)\n",
    "data1['CURRENT_RATING'].replace(dict_rating, inplace=True)\n",
    "data1['LAST_RATING'].replace(dict_rating, inplace=True)\n",
    "data1['MANAGER_RATING'].replace(dict_rating, inplace=True)\n",
    "data1['HIGHEST_EDUCATION'].replace(dict_education, inplace=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion of object to category \n",
    "for i in data1:\n",
    "    if data1[i].dtype==object:\n",
    "        data1[i] = data1[i].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llisst = ['EMPLOYEE_AGE','NO_OF_LEAVES','TOTAL_REPORTEES','NO_OF_MANAGER_REPORTEES','TECHM_EXPERIENCE','TOTAL_EXPERIENCE','NO_OF_PREVIOUS_EMPLOYERS','LAST_ONSITE_TRAVEL_DAYS','TOTAL_PROMOTIONS','MANAGER_RATING','CURRENT_RATING','LAST_RATING','REWARDS','QUARTILE','TENURE_LAST_EMPLOPYER','LATEST_HIKE_PERCENT','LAST_HIKE_PERCENT','HIGHEST_EDUCATION','CITY_FLAG','Month']\n",
    "for j in llisst:\n",
    "      data1[j] = data1[j].fillna(0.0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = data1.corr()\n",
    "f,ax=plt.subplots(figsize=(10,10))\n",
    "#b.heatmap(corr,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".1f\",ax=ax)\n",
    "sb.heatmap(corr,annot=False,linewidths=0.5,linecolor=\"green\",fmt=\".1f\",ax=ax)\n",
    "#plt.show()\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#data1 = pd.get_dummies(data1, columns=['carrier'], prefix = ['carrier'])\n",
    "data1 = pd.get_dummies(data1, columns=['GENDER','DEPARTMENT_ID','PRIMARY_SKILL','SECONDARY_SKILL','CURRENT_LOCATION_CITY','BASE_LOCATION_CITY','HIRE_LOCATION_CITY','RESIGNATION_FLG','BILLABILITY_STATUS','ALL_PREVIOUS_RAINGS','OFFICERCODE_DESC','GRIEVANCE_FLG'], prefix = ['GENDER','DEPARTMENT_ID','PRIMARY_SKILL','SECONDARY_SKILL','CURRENT_LOCATION_CITY','BASE_LOCATION_CITY','HIRE_LOCATION_CITY','RESIGNATION_FLG','BILLABILITY_STATUS','ALL_PREVIOUS_RAINGS','OFFICERCODE_DESC','GRIEVANCE_FLG'])\n",
    "\n",
    "print(data1.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dft = data1.pop('HIRE_EXIT_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['HIRE_EXIT_DATE']=dft\n",
    "\n",
    "\n",
    "dft = data1.pop('LAST_PROMOTION_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['LAST_PROMOTION_DATE']=dft\n",
    "\n",
    "dft = data1.pop('MANAGER_LAST_PROMOTION') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['MANAGER_LAST_PROMOTION']=dft\n",
    "\n",
    "\n",
    "dft = data1.pop('LAST_ONSITE_TRAVEL_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['LAST_ONSITE_TRAVEL_DATE']=dft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to put target variable in last\n",
    "dft = data1.pop('TARGET_VALUE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['TARGET_VALUE']=dft # add b series as a 'new' column.\n",
    "#df['x']=df2 # add b series as a 'new' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = data1.iloc[:,0:35]\n",
    "target = data1.iloc[:,-1]\n",
    "#splitting the dataset into training and testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "#dataframe = pandas.read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = features.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to put target variable in last\n",
    "dft = data1.pop('TARGET_VALUE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['TARGET_VALUE']=dft # add b series as a 'new' column.\n",
    "#df['x']=df2 # add b series as a 'new' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dft = data1.pop('HIRE_EXIT_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['HIRE_EXIT_DATE']=dft\n",
    "\n",
    "\n",
    "dft = data1.pop('LAST_PROMOTION_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['LAST_PROMOTION_DATE']=dft\n",
    "\n",
    "dft = data1.pop('MANAGER_LAST_PROMOTION') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['MANAGER_LAST_PROMOTION']=dft\n",
    "\n",
    "\n",
    "dft = data1.pop('LAST_ONSITE_TRAVEL_DATE') # remove column b and store it in df1\n",
    "#df2 = df.pop('x') # remove column x and store it in df2\n",
    "data1['LAST_ONSITE_TRAVEL_DATE']=dft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.80)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,0:34], dataset[train_size:len(dataset),:-1]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 2, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 2, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=2, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train,feature_test,target_train, target_test  = train_test_split(features,target,test_size=0.2,random_state=42)\n",
    "sc = StandardScaler()\n",
    "feature_train = sc.fit_transform(feature_train)\n",
    "feature_test = sc.transform(feature_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the training set\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n",
    "# So for each element of training set, we have 60 previous training set elements \n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60,2769):\n",
    "    X_train.append(training_set_scaled[i-60:i,0])\n",
    "    y_train.append(training_set_scaled[i,0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping X_train for efficient modelling\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "# The LSTM architecture\n",
    "regressor = Sequential()\n",
    "# First LSTM layer with Dropout regularisation\n",
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Second LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Third LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Fourth LSTM layer\n",
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n",
    "# Fitting to the training set\n",
    "regressor.fit(X_train,y_train,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#features = data.iloc[:,[1,3,4,7,8,9,10,12,13,16,17,18,29]]\n",
    "#target = data.iloc[:,]\n",
    "features = data1.iloc[:,0:34]\n",
    "target = data1.iloc[:,-1]\n",
    "#splitting the dataset into training and testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    " features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We will use three methods for feature selection:\n",
    "\n",
    "Remove collinear features\n",
    "Remove features with greater than a threshold percentage of missing values\n",
    "Keep only the most relevant features using feature importances from a model'''\n",
    "#Identify Correlated Variables\n",
    "# Threshold for removing correlated variables\n",
    "threshold = 90\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = (features.corr().abs())*100\n",
    "corr_matrix.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop(['TOTAL_EXPERIENCE', 'CURRENT_LOCATION_CITY', 'HIRE_LOCATION_CITY'], axis=1)\n",
    "#target = target\n",
    "#df = pd.DataFrame.drop(['Meter ID', 'abc'], axis=1)\n",
    "\n",
    "print('Training shape: ', features.shape)\n",
    "print('Testing shape: ', target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as  tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to help out with\n",
    "def plot_predictions(test,predicted):\n",
    "    plt.plot(test, color='red',label='Real ')\n",
    "    plt.plot(predicted, color='blue',label='Predicted ')\n",
    "    plt.title('Attrition Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Attrition')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def return_rmse(test,predicted):\n",
    "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
    "    print(\"The root mean squared error is {}.\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# First, we get the data\n",
    "dataset = pd.read_csv('../input/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\n",
    "dataset.head()\n",
    "'''\n",
    "d1 = pd.read_csv('/dbfs/FileStore/tables/PRED_31MAR17.csv',index_col='HIRE_EXIT_DATE', parse_dates = ['HIRE_EXIT_DATE','LAST_PROMOTION_DATE','MANAGER_LAST_PROMOTION','LAST_ONSITE_TRAVEL_DATE'])\n",
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "training_set = dataset[:'2016'].iloc[:,1:2].values\n",
    "test_set = dataset['2017':].iloc[:,1:2].values\n",
    "# We have chosen 'High' attribute for prices. Let's see what it looks like\n",
    "dataset[\"High\"][:'2016'].plot(figsize=(16,4),legend=True)\n",
    "dataset[\"High\"]['2017':].plot(figsize=(16,4),legend=True)\n",
    "plt.legend(['Training set (Before 2017)','Test set (2017 and beyond)'])\n",
    "plt.title('IBM stock price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(feature_train.shape[1], feature_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# fit\n",
    "history = model.fit(feature_train, target_train, epochs=1, batch_size=10, validation_data=(feature_test, target_test), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulid model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(feature_train.shape[1:]), activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('Minst_Lstm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators = 800, criterion = 'entropy', random_state = 0)\n",
    "random_forest.fit(feature_train, target_train)\n",
    "\n",
    "def plot_feature_importances(importances, features):\n",
    "    # get the importance rating of each feature and sort it\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    # make a plot with the feature importance\n",
    "    plt.figure(figsize=(12,14), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.grid()\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], height=0.8, color='mediumvioletred', align='center')\n",
    "    plt.axvline(x=0.03)\n",
    "    plt.yticks(range(len(indices)), list(features))\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "    display()\n",
    "\n",
    "plot_feature_importances(random_forest.feature_importances_, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Logistic regression\n",
    "classifier = LogisticRegression(random_state= 0)\n",
    "classifier.fit(feature_train,target_train)\n",
    "pred = classifier.predict(feature_test)\n",
    "print(pred)\n",
    "cm = confusion_matrix(target_test,pred)\n",
    "print(cm)\n",
    "score = classifier.score(feature_test,target_test)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classs =  KNeighborsClassifier(n_neighbors = 5,p = 2)\n",
    "classs.fit(feature_train,target_train)\n",
    "pred1  = classs.predict(feature_test)\n",
    "score1 = classs.score(feature_test,target_test)\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis - Numerical Features\n",
    "#Let's visualize data distribution of numerical features. Usually, prediction model works well if the data distribution is normal distribution. So, if there are skewed data distirubtion then we can make them normal distribution through log transformations.\n",
    "\n",
    "#without log transformation\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "# Histogram Plot for Employee Age\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(emp_viz_df['EMPLOYEE_AGE'], bins=15, color='green')\n",
    "plt.title('Distribution of Employee Age')\n",
    "plt.xlabel(\"Employee Age\")\n",
    "\n",
    "# Histogram Plot for employee Monthly Income\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(emp_viz_df['TOTAL_PROMOTIONS'], bins=30, color='red')\n",
    "plt.title('Distribution of Employee Monthly Income')\n",
    "plt.xlabel(\"Employee Monthly Income\")\n",
    "\n",
    "fig.tight_layout();\n",
    "display()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#with log \n",
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "# Histogram Plot for Employee Age\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(np.log1p(emp_viz_df['EMPLOYEE_AGE']), bins=20, color='green')\n",
    "plt.title('Distribution of Employee Age')\n",
    "plt.xlabel(\"Log of Employee Age\")\n",
    "\n",
    "# Histogram Plot for employee Monthly Income\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.log1p(emp_viz_df['TOTAL_PROMOTIONS']), bins=15, color='red')\n",
    "plt.title('Distribution of Employee Monthly Income')\n",
    "plt.xlabel(\"Log of Employee Monthly Income\")\n",
    "\n",
    "fig.tight_layout();\n",
    "display()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Skewness in employee Age feature: ', emp_viz_df['EMPLOYEE_AGE'].skew())\n",
    "print('Skewness in employee Monthly Income feature: ', emp_viz_df['TOTAL_PROMOTIONS'].skew())\n",
    "\"\"\"\n",
    "\"\"\"We can observe above that without log transformation, \"Age\" feature looks normally distributed more than \"MonthlyIncome\" feature.\n",
    "Hence, after log transformation, there is hardly any change in \"Age\" distributions but there is significant transformation in \"MonthlyIncome\" feature.\n",
    "This can also be confirmed by checking the skewness of each feature by applying numpy skew metric and higher the skewness, higher the value.\n",
    "So, we will use some threshold value of skewness to carry out log transformation on features with high skewness.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic statistics of categorical features\n",
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  print(i.describe(include=[np.object]))\n",
    "#cat_col_names = data.select_dtypes(include=[np.object]).columns.tolist() # Get categorical feature names\n",
    "#cat_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attrition Target Variable Distribution\n",
    "lllist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in lllist:\n",
    "  attrition_freq = i[['TARGET_VALUE']].apply(lambda x: x.value_counts())\n",
    "  attrition_freq['frequency_percent'] = round((100 * attrition_freq / attrition_freq.sum()),2)\n",
    "\n",
    "  print(attrition_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(variable):\n",
    "  for i in llist:\n",
    "      print(i[variable].value_counts(),(i[i.TARGET_VALUE=='I'][variable].value_counts()*100)/i[variable].value_counts(),i[i.TARGET_VALUE=='I'][variable].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution('DEPARTMENT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  print(distribution('OFFICERCODE_DESC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"To understand how each features are impacting the attrition indicator, we need to first handle the null / missing values, otherwise our observations might not be accurate and will lead to wrong conclusions.\n",
    "\"\"\"\n",
    "for i in llist:\n",
    "    null_feat_df = pd.DataFrame()\n",
    "    null_feat_df['Null Count'] = i.isnull().sum().sort_values(ascending=False)\n",
    "    null_feat_df['Null Pct'] = null_feat_df['Null Count'] / float(len(i))\n",
    "\n",
    "    null_feat_df = null_feat_df[null_feat_df['Null Pct'] > 0]\n",
    "\n",
    "\n",
    "    total_null_feats = null_feat_df.shape[0]\n",
    "    null_feat_names = null_feat_df.index\n",
    "    print('Total number of features having null values: ', total_null_feats)\n",
    "    del null_feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling with 0\n",
    "for i in llist:\n",
    "    i[\"LATEST_HIKE_PERCENT\"].fillna(\"0\", inplace = True) \n",
    "    i[\"LAST_HIKE_PERCENT\"].fillna(\"0\", inplace = True)\n",
    "\n",
    "    i['EMPLOYEE_AGE'].fillna(i['EMPLOYEE_AGE'].mode()[0], inplace=True) \n",
    "#EMPLOYEE_AGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1) \n",
    "plt.subplot(221) \n",
    "data['GENDER'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender') \n",
    "plt.subplot(222) \n",
    "data['BAND'].value_counts(normalize=True).plot.bar(title= 'Band') \n",
    "plt.subplot(223) \n",
    "data['REWARDS'].value_counts(normalize=True).plot.bar(title= 'REWARDS') \n",
    "plt.subplot(224) \n",
    "#plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregating data set based on Attrition value\n",
    "\n",
    "#Attrition across Job Roles\n",
    "fig, ax6 = plt.subplots(1,2, figsize=(24,10))\n",
    "attr_yes = data[data.TARGET_VALUE=='I'] #subset with Attrition = Yes\n",
    "attr_no = data[data.TARGET_VALUE=='A'] ##subset with Attrition = No\n",
    "sb.countplot(x='BAND', hue='GENDER', data = attr_yes, palette=\"Set3\", ax = ax6[1])\n",
    "sb.countplot(x='BILLABILITY_STATUS', hue='GENDER', data = attr_yes,palette=\"Set3\", ax = ax6[0])\n",
    "plt.show()\n",
    "display()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def plot_count(feature, title,size=1):\n",
    "    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n",
    "    total = float(len(data))\n",
    "    g = sb.countplot(data[feature], order = data[feature].value_counts(normalize=True).index[:20], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "    plt.show()   \n",
    "    display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('CURRENT_RATING','current rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "plt.figure(figsize=(15, 8))\n",
    "cols = ['LAST_ONSITE_TRAVEL_DAYS', 'REWARDS', 'TOTAL_PROMOTIONS', 'TENURE_LAST_EMPLOPYER', 'NO_OF_PREVIOUS_EMPLOYERS']\n",
    "uniques = [len(data[col].unique()) for col in cols]\n",
    "sb.set(font_scale=1.2)\n",
    "sb.color_palette(\"Blues\")\n",
    "ax = sb.barplot(cols, uniques,log=True)\n",
    "ax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 10,\n",
    "            uniq,\n",
    "            ha=\"center\") \n",
    "# for col, uniq in zip(cols, uniques):\n",
    "#     ax.text(col, uniq, uniq, color='black', ha=\"center\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divisionplot(variable):\n",
    "  var=pd.crosstab(data[variable],data['QUARTILE'])\n",
    "  var.div(var.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize = (10,4)) \n",
    "  #plt.show()\n",
    "  display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "divisionplot('BAND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sb.jointplot(x=\"TOTAL_PROMOTIONS\", y=\"TOTAL_EXPERIENCE\", data=data,  ratio=3, color=\"r\")\n",
    "plt.show()\n",
    "display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_data = fd4.copy()\n",
    "listdf = [fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in listdf:\n",
    "  print(i['CURRENT_RATING'].value_counts() , i['CURRENT_RATING'][i['TARGET_VALUE'] == 'I'].value_counts())\n",
    "  #print(i['CURRENT_RATING'][i['TARGET_VALUE'] == 'I'].value_counts())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['CURRENT_RATING']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['CURRENT_RATING']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AGE_Interval'] = pd.cut(data['EMPLOYEE_AGE'], 6, labels=['19-25','25-35', '35-45', '45-55', '55-65','65+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age=pd.DataFrame(data.groupby(\"AGE_Interval\")[[\"TOTAL_REPORTEES\",\"NO_OF_LEAVES\",\"NO_OF_MANAGER_REPORTEES\",\"TECHM_EXPERIENCE\",\"TOTAL_EXPERIENCE\",\"LAST_HIKE_PERCENT\",\"NO_OF_PREVIOUS_EMPLOYERS\",\"TENURE_LAST_EMPLOPYER\",\"TOTAL_PROMOTIONS\",\"LAST_ONSITE_TRAVEL_DAYS\",\"REWARDS\",\"LATEST_HIKE_PERCENT\",\"HIGHEST_EDUCATION\"]].mean())\n",
    "age[\"Count\"]=data.AGE_Interval.value_counts(dropna=False)\n",
    "age.reset_index(level=0, inplace=True)\n",
    "age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax=sb.barplot(x=age.AGE_Interval,y=age.Count)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Age Counts\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['AGE_Interval'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax=sb.barplot(x=age.AGE_Interval,y=age.TOTAL_PROMOTIONS,palette = sb.cubehelix_palette(len(age.index)))\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"TOTAL_PROMOTIONS\")\n",
    "plt.title(\"TOTAL_PROMOTIONS According to Age\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "labels=data.AGE_Interval.value_counts().index\n",
    "# colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\"]\n",
    "#explode=[0,0,0,0,0,0]\n",
    "sizes=(data[data.TARGET_VALUE=='I'].AGE_Interval.value_counts()*100)/data.AGE_Interval.value_counts().values\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\")\n",
    "plt.title(\"AGE interval Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "age1=pd.DataFrame(data.groupby(\"EMPLOYEE_AGE\")[[\"TOTAL_REPORTEES\",\"NO_OF_LEAVES\",\"NO_OF_MANAGER_REPORTEES\",\"TECHM_EXPERIENCE\",\"TOTAL_EXPERIENCE\",\"LAST_HIKE_PERCENT\",\"NO_OF_PREVIOUS_EMPLOYERS\",\"TENURE_LAST_EMPLOPYER\",\"TOTAL_PROMOTIONS\",\"LAST_ONSITE_TRAVEL_DAYS\",\"REWARDS\",\"LATEST_HIKE_PERCENT\",\"HIGHEST_EDUCATION\"]].mean())\n",
    "age1[\"Count\"]=data.EMPLOYEE_AGE.value_counts(dropna=False)\n",
    "age1.reset_index(level=0, inplace=True)\n",
    "age1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenorm=age1.apply(lambda x: x/max(x))\n",
    "agenorm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageb=data.EMPLOYEE_AGE.value_counts().index\n",
    "f,ax=plt.subplots(figsize=(15,15))\n",
    "sb.pointplot(y=agenorm.LAST_HIKE_PERCENT,x=ageb,color=\"purple\",alpha=0.8)\n",
    "sb.pointplot(y=agenorm.TOTAL_PROMOTIONS,x=ageb,color=\"sandybrown\",alpha=0.8)\n",
    "plt.text(5,0.65,\"LAST_HIKE_PERCENT\",color=\"purple\",fontsize=15,style=\"italic\")\n",
    "plt.text(5,0.63,\"TOTAL_PROMOTIONS\",color=\"sandybrown\",fontsize=15,style=\"italic\")\n",
    "plt.xlabel(\"Age\",fontsize=15,color=\"darkred\")\n",
    "plt.ylabel(\"Values\",fontsize=15,color=\"darkred\")\n",
    "plt.title(\"LAST_HIKE_PERCENTe VS JTOTAL_PROMOTIONS\",fontsize=15,color=\"darkred\")\n",
    "plt.grid()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageb=data.EMPLOYEE_AGE.value_counts().index\n",
    "f,ax=plt.subplots(figsize=(15,15))\n",
    "sb.pointplot(y=agenorm.LATEST_HIKE_PERCENT,x=ageb,color=\"purple\",alpha=0.8)\n",
    "sb.pointplot(y=agenorm.TOTAL_PROMOTIONS,x=ageb,color=\"sandybrown\",alpha=0.8)\n",
    "plt.text(5,0.65,\"LATEST_HIKE_PERCENT\",color=\"purple\",fontsize=15,style=\"italic\")\n",
    "plt.text(5,0.63,\"TOTAL_PROMOTIONS\",color=\"sandybrown\",fontsize=15,style=\"italic\")\n",
    "plt.xlabel(\"Age\",fontsize=15,color=\"darkred\")\n",
    "plt.ylabel(\"Values\",fontsize=15,color=\"darkred\")\n",
    "plt.title(\"LAST_HIKE_PERCENTe VS JTOTAL_PROMOTIONS\",fontsize=15,color=\"darkred\")\n",
    "plt.grid()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "sb.kdeplot(agenorm.REWARDS,agenorm.LAST_ONSITE_TRAVEL_DAYS,shade=True,cut=1)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "sb.kdeplot(agenorm.TOTAL_PROMOTIONS,agenorm.LAST_ONSITE_TRAVEL_DAYS,shade=True,cut=1)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "promotions=pd.DataFrame(data.groupby(\"BAND\").TOTAL_PROMOTIONS.mean().sort_values(ascending=False))\n",
    "plt.figure(figsize=(5,5))\n",
    "ax=sb.barplot(x=promotions.index,y=promotions.TOTAL_PROMOTIONS)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"BAND\")\n",
    "plt.ylabel(\"TOTAL_PROMOTIONS\")\n",
    "plt.title(\"BAND with TOTAL_PROMOTIONS\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "band=pd.DataFrame(data.groupby(\"BAND\")[\"EMPLOYEE_AGE\",\"NO_OF_LEAVES\",\"NO_OF_PREVIOUS_EMPLOYERS\",\"NO_OF_MANAGER_REPORTEES\",\"LAST_ONSITE_TRAVEL_DAYS\",\"LATEST_HIKE_PERCENT\",\"LAST_HIKE_PERCENT\",\"NO_OF_MANAGER_REPORTEES\",\"TENURE_LAST_EMPLOPYER\",\"TECHM_EXPERIENCE\",\"TOTAL_EXPERIENCE\"].mean())\n",
    "band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (9,10))\n",
    "sb.barplot(x=band.LATEST_HIKE_PERCENT,y=band.index,color='green',alpha = 0.5,label='LATEST_HIKE_PERCENT' )\n",
    "sb.barplot(x=band.LAST_ONSITE_TRAVEL_DAYS,y=band.index,color='blue',alpha = 0.7,label='LAST_ONSITE_TRAVEL_DAYS')\n",
    "sb.barplot(x=band.TECHM_EXPERIENCE,y=band.index,color='cyan',alpha = 0.6,label='Years At Company')\n",
    "sb.barplot(x=band.NO_OF_PREVIOUS_EMPLOYERS,y=band.index,color='yellow',alpha = 0.6,label='NO_OF_PREVIOUS_EMPLOYERS')\n",
    "sb.barplot(x=band.NO_OF_LEAVES,y=band.index,color='red',alpha = 0.6,label='NO_OF_LEAVES')\n",
    "\n",
    "ax.legend(loc='right',frameon = True)     \n",
    "ax.set(xlabel='Values', ylabel='band',title = \"band with Different Features\")\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.pairplot(data, vars=[\"TOTAL_PROMOTIONS\", \"REWARDS\"],hue=\"BAND\",size=5)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "sb.boxplot(x=\"GENDER\",y=\"EMPLOYEE_AGE\",hue=\"QUARTILE\",data=data,palette=\"Paired\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "ax.legend(frameon=False, loc='lower center', ncol=2)\n",
    "\n",
    "sb.boxplot(x=\"GENDER\",y=\"EMPLOYEE_AGE\",hue=\"BAND\" ,data=data,palette=\"hls\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "sb.pointplot(x=\"GENDER\", y=\"TECHM_EXPERIENCE\", hue=\"TARGET_VALUE\", data=data,\n",
    "              palette={\"I\": \"blue\", \"A\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"]);\n",
    "display()\n",
    "## Inference\n",
    "# Males and Females within the ranges for 'TotalWorkingYears' of 11 to 13 are less likely to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TECHM_Interval'] = pd.cut(data['TECHM_EXPERIENCE'], 6, labels=['0-5','5-10', '10-15', '15-20', '20-25','25+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "#plt.subplots(figsize=(15,5))\n",
    "sb.countplot(data.TECHM_Interval)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['HIGHEST_EDUCATION'].value_counts()\n",
    "\n",
    "\n",
    "#print(data['HIGHEST_EDUCATION'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['EDUCATION_CATEGORIES'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (11,5))\n",
    "#plt.subplots(figsize=(15,5))\n",
    "sb.countplot(data.EDUCATION_CATEGORIES)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data.EDUCATION_CATEGORIES.value_counts().index\n",
    "colors=[\"olive\",\"orange\",\"hotpink\",\"slateblue\",\"y\",\"lime\",\"b\"]\n",
    "explode=[1,0,0,0,1,1]\n",
    "sizes=data.EDUCATION_CATEGORIES.value_counts() \n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes,labels=labels,colors=colors,autopct=\"%1.1f%%\")\n",
    "plt.title(\"Education Field Counts\",color=\"saddlebrown\",fontsize=15)\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  i['LeavesInterval'] = pd.cut(i['NO_OF_LEAVES'][i['NO_OF_LEAVES']<50], 5, labels=['<10', '<20', '<30', '<40','<50+'])\n",
    "  freq = i[['LeavesInterval']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['LeavesInterval']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['LeavesInterval'].value_counts()\n",
    "data['LeavesInterval'] = pd.cut(i['NO_OF_LEAVES'][i['NO_OF_LEAVES']<50], 5, labels=['<10', '<20', '<30', '<40','<50+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['DEPARTMENT_ID']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['DEPARTMENT_ID']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  #print(freq)\n",
    "  print(freq[\"frequency_percent\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library and dataset\n",
    "#import seaborn as sns\n",
    "#df = sns.load_dataset('iris')\n",
    "f,ax = plt.subplots(figsize = (15,10))\n",
    "# plot of 2 variables\n",
    "sb.kdeplot(data['TOTAL_REPORTEES'], shade=True, color=\"r\")\n",
    "sb.kdeplot(data['NO_OF_MANAGER_REPORTEES'], shade=True, color=\"b\")\n",
    "#sns.plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library and dataset\n",
    "#import seaborn as sns\n",
    "#df = sns.load_dataset('iris')\n",
    "f,ax = plt.subplots(figsize = (5,5))\n",
    "# plot of 2 variables\n",
    "sb.kdeplot(data['TOTAL_PROMOTIONS'], shade=True, color=\"r\")\n",
    "sb.kdeplot(data['REWARDS'], shade=True, color=\"b\")\n",
    "#sns.plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['HIRE_LOCATION_CITY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = pd.Index(data.CURRENT_LOCATION_CITY)\n",
    "idx2 = pd.Index(data.BASE_LOCATION_CITY)\n",
    "idx3 = pd.Index(data.HIRE_LOCATION_CITY)\n",
    "#main_data['unique'] = idx1.difference(idx2).values\n",
    "print(idx1.difference(idx2).values)\n",
    "print(idx2.difference(idx1).values)\n",
    "print(idx3.difference(idx1).values)\n",
    "#print(idx3.difference(idx2).values)\n",
    "#array([3, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['HIRE_LOCATION_CITY']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['HIRE_LOCATION_CITY']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST_RATING,CURRENT_RATING\n",
    "for i in listdf:\n",
    "  freq = i[['HIRE_EXIT_DATE']].apply(lambda x: x.value_counts())\n",
    "  attr_yes_rating =i[['HIRE_EXIT_DATE']][i['TARGET_VALUE'] == 'I'].apply(lambda x: x.value_counts()) \n",
    "  freq['frequency_percent'] = round((100 * attr_yes_rating / freq.sum()),2)\n",
    "  #display()\n",
    "  print(freq)\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['LeavesInterval','TECHM_Interval','CURRENT_RATING','AGE_Interval', 'TOTAL_PROMOTIONS','DEPARTMENT_ID ','QUARTILE']\n",
    "fig=plt.subplots(figsize=(10,15))\n",
    "for i, j in enumerate(features):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.subplots_adjust(hspace = 1.0)\n",
    "    sb.countplot(x=j,data = data)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"No. of employee\")\n",
    "    display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart of workers\n",
    "labels = ['M', 'F']\n",
    "sizes = [data['GENDER'].value_counts()[0],\n",
    "         data['GENDER'].value_counts()[1]\n",
    "        ]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True)\n",
    "ax1.axis('equal')\n",
    "plt.title('Gender Pie Chart', fontsize=20)\n",
    "plt.show()\n",
    "display()\n",
    "\n",
    "## Inference\n",
    "# Proportion of male is higher than female associates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.subplots(figsize=(5,5))\n",
    "sb.factorplot(data=data,y='TOTAL_PROMOTIONS',x='BAND',size=7,aspect=2,kind='point')\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plotting the KDEplots\n",
    "f, axes = plt.subplots(3, 3, figsize=(10, 8), \n",
    "                       sharex=False, sharey=False)\n",
    "\n",
    "# Defining our colormap scheme\n",
    "s = np.linspace(0, 3, 7)\n",
    "cmap = sb.cubehelix_palette(start=0.0, light=1, as_cmap=True)\n",
    "\n",
    "# Generate and plot\n",
    "x = data['EMPLOYEE_AGE'].values\n",
    "y = data['TECHM_EXPERIENCE'].values\n",
    "sb.kdeplot(x, y, cmap=cmap, shade=True, cut=5, ax=axes[0,0])\n",
    "axes[0,0].set( title = 'Age Vs TechMExperience')\n",
    "#display()\n",
    "cmap = sb.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True)\n",
    "\n",
    "# Generate and plot\n",
    "x = data['EMPLOYEE_AGE'].values\n",
    "y = data['NO_OF_LEAVES'].values\n",
    "sb.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,1])\n",
    "axes[0,1].set( title = 'Age Vs no_of_leaves')\n",
    "#display()\n",
    "\n",
    "cmap = sb.cubehelix_palette(start=0.666666666667, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = data['NO_OF_PREVIOUS_EMPLOYERS'].values\n",
    "y = data['EMPLOYEE_AGE'].values\n",
    "sb.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,2])\n",
    "axes[0,2].set( title = 'No_of_prev_employers Vs Age')\n",
    "display()\n",
    "'''\n",
    "cmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['DailyRate'].values\n",
    "y = attrition['DistanceFromHome'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,0])\n",
    "axes[1,0].set( title = 'Daily Rate against DistancefromHome')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=1.333333333333, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['DailyRate'].values\n",
    "y = attrition['JobSatisfaction'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,1])\n",
    "axes[1,1].set( title = 'Daily Rate against Job satisfaction')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['YearsAtCompany'].values\n",
    "y = attrition['JobSatisfaction'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,2])\n",
    "axes[1,2].set( title = 'Daily Rate against distance')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=2.0, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['YearsAtCompany'].values\n",
    "y = attrition['DailyRate'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,0])\n",
    "axes[2,0].set( title = 'Years at company against Daily Rate')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=2.333333333333, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['RelationshipSatisfaction'].values\n",
    "y = attrition['YearsWithCurrManager'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,1])\n",
    "axes[2,1].set( title = 'Relationship Satisfaction vs years with manager')\n",
    "\n",
    "cmap = sns.cubehelix_palette(start=2.666666666667, light=1, as_cmap=True)\n",
    "# Generate and plot\n",
    "x = attrition['WorkLifeBalance'].values\n",
    "y = attrition['JobSatisfaction'].values\n",
    "sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,2])\n",
    "axes[2,2].set( title = 'WorklifeBalance against Satisfaction')\n",
    "f.tight_layout()\n",
    "dispaly()\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_small = data# train.sample(frac=0.2).copy() # not small for now\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "sb.kdeplot(train_small.loc[data['BILLABILITY_STATUS'] == 'N', 'EMPLOYEE_AGE'], ax=ax[0], label='NoPay(0)')\n",
    "sb.kdeplot(train_small.loc[data['BILLABILITY_STATUS'] == 'Y', 'EMPLOYEE_AGE'], ax=ax[0], label='HasPay(1)')\n",
    "\n",
    "#\\train_small.loc[data['HasDetections'] == 0, 'DefaultBrowsersIdentifier'].hist(ax=ax[1])\n",
    "#train_small.loc[train['HasDetections'] == 1, 'DefaultBrowsersIdentifier'].hist(ax=ax[1])\n",
    "#ax[1].legend(['NoDetection(0)', 'HasDetection(1)'])\n",
    "\n",
    "plt.show()\n",
    "display()\n",
    "\"\"\"\n",
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'''\n",
    "# Segregating data set based on Attrition value\n",
    "attr_yes = data[data.TARGET_VALUE=='I'] #subset with Attrition = Yes\n",
    "attr_no = data[data.TARGET_VALUE=='A'] ##subset with Attrition = No\n",
    "'''\n",
    "attrition = data[(data['TARGET_VALUE'] == 'A' )]\n",
    "no_attrition = data[(data['TARGET_VALUE'] == 'I')]\n",
    "'''\n",
    "# Segregating data set based on Attrition value\n",
    "attr_yes = data[data.TARGET_VALUE=='I'] #subset with Attrition = Yes\n",
    "attr_no = data[data.TARGET_VALUE=='A'] ##subset with Attrition = No\n",
    "#Add columns to store discrete variables for Salary range and alcohol levels\n",
    "#Adding discrete valriables\n",
    "data['LeavesInterval'] = pd.cut(data['NO_OF_LEAVES'], 5, labels=['<9', '<15', '<24', '<32', '<33+'])\n",
    "data['TechMExpLvl'] = pd.cut(data['TECHM_EXPERIENCE'], 5, labels=['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5'])\n",
    "data['traveldate'] = pd.cut(data['LAST_ONSITE_TRAVEL_DAYS'], 5, labels=['0to3', '3to6', '6to9', '9to12', '12+'])\n",
    "\n",
    "\n",
    "\n",
    "#Factor Plot\n",
    "sb.factorplot(x =   'TARGET_VALUE',     # Categorical\n",
    "               y =   'NO_OF_LEAVES',          # Continuous\n",
    "               hue = 'DEPARTMENT_ID',   # Categorical\n",
    "               col = 'LeavesInterval',   # Categorical for graph columns\n",
    "               col_wrap=3,           # Wrap facet after two axes\n",
    "               kind = 'box',\n",
    "               data = data)\n",
    "plt.show()\n",
    "display()\n",
    "\n",
    "\n",
    "# Attrition count across Daily Rate, Years in company, Years since last promotion and level of stock options\n",
    "fig, ax2 = plt.subplots(2,2, figsize=(10,10))\n",
    "sb.countplot(x='LeavesInterval', data=attr_yes, ax = ax2[0,0])\n",
    "sb.countplot(x='TechMExpLvl', data=attr_yes, ax = ax2[0,1])\n",
    "sb.countplot(x='traveldate', data=attr_yes, ax = ax2[1,0])\n",
    "#sb.countplot(x='StockOptionLevel', data=attr_yes, ax = ax2[1,1])\n",
    "plt.show()\n",
    "display()\n",
    "'''\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#Count Plot\n",
    "# Attrition across Education level and corresponding percentage across the total number of employees\n",
    "total = data.shape[0] \n",
    "hrfig = sb.countplot(x='HIGHEST_EDUCATION', hue = 'TARGET_VALUE', data = data)\n",
    "#Above graph showcases the percentages of the employees across Education levels and corresponding attri\n",
    "for p in hrfig.patches:\n",
    "    height = p.get_height()\n",
    "    hrfig.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}'.format(height*100/total),\n",
    "            ha=\"center\") \n",
    "plt.show()\n",
    "display()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "dt = datetime.combine(date.today(), datetime.min.time())\n",
    "print(dt)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Attrition distribution bar plot\n",
    "\n",
    "plot = attrition_freq[['frequency_percent']].plot(kind=\"bar\");\n",
    "plot.set_title(\"Attrition Distribution\", fontsize=40);\n",
    "plot.grid(color='lightgray', alpha=0.5);\n",
    "display()\n",
    "\"\"\"\n",
    "\"\"\"Just by a quick inspection of the counts of the number of 'Yes' and 'No' in the target variable tells us that there is quite a large skew in target variable.\n",
    "Therefore we have to keep in mind that there is quite a big imbalance in our target variable. Many statistical techniques have been put forth to treat imbalances in data (oversampling or undersampling).\n",
    "In this notebook, I will use an oversampling technique known as SMOTE to treat this imbalance.\n",
    "We will see the prediction model with and without SMOTE treatment for imbalance class issue.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Step 4 - Feature Engineering\n",
    "Feature engineering is one aspect which provided a huge impact on the outcome rather than the model. Here, we try at creating new features with the existing variables we have based on my assumptions.\n",
    "\n",
    "Step 4.1 - Addition of New Features\n",
    "Tenure per job: Usually, people who have worked with many companies but for small periods at every organization tend to leave early as they always need a change of Organization to keep them going.\n",
    "Years without Change: For any person, a change either in role or job level or responsibility is needed to keep the work exciting to continue. We create a variable to see how many years it has been for an employee without any sort of change using Promotion, Role and Job Change as a metric to cover different variants of change.\n",
    "Compensation Ratio: Compa Ratio is the ratio of the actual pay of an Employee to the midpoint of a salary range. The salary range can be that of his/her department or organization or role. The benchmark numbers can be a organizationâ€™s pay or Industry average.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "emp_proc_df = data.copy() # Copy cleaned dataset for feature engineering\n",
    "\n",
    "emp_proc_df['TenurePerJob'] = 0\n",
    "\n",
    "for i in range(0, len(emp_proc_df)):\n",
    "    if emp_proc_df.loc[i,'NO_OF_PREVIOUS_EMPLOYERS'] > 0:\n",
    "        emp_proc_df.loc[i,'TenurePerJob'] = emp_proc_df.loc[i,'TOTAL_EXPERIENCE'] / emp_proc_df.loc[i,'NO_OF_PREVIOUS_EMPLOYERS']\n",
    "\"\"\"\n",
    "emp_proc_df['YearWithoutChange1'] = emp_proc_df['YearsInCurrentRole'] - emp_proc_df['YearsSinceLastPromotion']\n",
    "emp_proc_df['YearWithoutChange2'] = emp_proc_df['TotalWorkingYears'] - emp_proc_df['YearsSinceLastPromotion']\n",
    "\n",
    "monthly_income_median = np.median(emp_proc_df['MonthlyIncome'])\n",
    "emp_proc_df['CompRatioOverall'] = emp_proc_df['MonthlyIncome'] / monthly_income_median\n",
    "\n",
    "print('Dataset dimension: {} rows, {} columns'.format(emp_proc_df.shape[0], emp_proc_df.shape[1]))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_col_names = emp_proc_df.columns.tolist()\n",
    "num_col_names = emp_proc_df.select_dtypes(include=[np.int64,np.float64]).columns.tolist() # Get numerical feature names\n",
    "\n",
    "# Preparing list of ordered categorical features\n",
    "for i in emp_proc_df:\n",
    "    if emp_proc_df[i].dtype==object:\n",
    "        num_cat_col_names = emp_proc_df[i]\n",
    "#num_cat_col_names = ['DEPARTMENT_ID', 'TARGET_VALUE', 'OFFICERCODE_DESC', 'ALL_PREVIOUS_RAINGS', 'JobSatisfaction',\n",
    "                     #'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance', 'StockOptionLevel']\n",
    "\n",
    "target = ['TARGET_VALUE']\n",
    "\n",
    "num_col_names = list(set(num_col_names) - set(num_cat_col_names)) # Numerical features w/o Ordered Categorical features\n",
    "cat_col_names = list(set(full_col_names) - set(num_col_names) - set(target)) # Categorical & Ordered Categorical features\n",
    "\n",
    "print('Total number of numerical features: ', len(num_col_names))\n",
    "print('Total number of categorical & ordered categorical features: ', len(cat_col_names))\n",
    "cat_emp_df = emp_proc_df[cat_col_names]\n",
    "num_emp_df = emp_proc_df[num_col_names]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Transform Numerical Features\n",
    "#In order to fix the skewness, letâ€™s take the log for all numerical features with an absolute skew greater than 80% (Note: log+1, to avoid division by zero issues).\n",
    "\n",
    "# Let's create dummy variables for each categorical attribute for training our calssification model\n",
    "for col in num_col_names:\n",
    "    if num_emp_df[col].skew() > 0.80:\n",
    "        num_emp_df[col] = np.log1p(num_emp_df[col])\n",
    "\n",
    "num_emp_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" #Transform Categorical Features\n",
    "Machine Learning model works only on numerical datasets, hence, we need to transform categorical features into numerical features.\n",
    "One of the best strategy is to convert each category value into a new column and assigns 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set.\n",
    "This approach is also called as \"One Hot Encoding\". We can use Pandas feature get_dummies to achieve this transformation.\n",
    "There is another way to handled ordered categorical feature is to give ordered value based on their definitions i.e Low-Meidum-High would be 1-2-3. We can try this approach some other time. But, this can be evaluated to check the performance of the model.\n",
    "\"\"\"\n",
    "# Let's create dummy variables for each categorical attribute for training our calssification model\n",
    "for col in cat_col_names:\n",
    "    col_dummies = pd.get_dummies(cat_emp_df[col], prefix=col)\n",
    "    cat_emp_df = pd.concat([cat_emp_df, col_dummies], axis=1)\n",
    "\n",
    "# Use the pandas apply method to numerically encode our attrition target variable\n",
    "attrition_target = emp_proc_df['TARGET_VALUE'].map({'I':1, 'A':0})\n",
    "\n",
    "# Drop categorical feature for which dummy variables have been created\n",
    "\n",
    "cat_emp_df.drop(cat_col_names, axis=1, inplace=True)\n",
    "cat_emp_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Correlation of Numerical Features against Attrition\n",
    "num_corr_df = num_emp_df[['EMPLOYEE_AGE', 'NO_OF_MANAGER_REPORTEES', 'TOTAL_REPORTEES']]\n",
    "corr_df = pd.concat([num_corr_df, attrition_target], axis=1)\n",
    "corr = corr_df.corr()\n",
    "display()\n",
    "plt.figure(figsize = (10, 8))\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sb.axes_style(\"white\")\n",
    "#sns.heatmap(data=corr, annot=True, mask=mask, square=True, linewidths=.5, vmin=-1, vmax=1, cmap=\"YlGnBu\")\n",
    "sb.heatmap(data=corr, annot=True, square=True, linewidths=.5, vmin=-1, vmax=1, cmap=\"YlGnBu\")\n",
    "plt.show()\n",
    "display()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Another way to check for correlation between attributes is to use Pandasâ€™ scatter_matrix function,\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(corr_df, figsize=(16, 10));\n",
    "display()\n",
    "\n",
    "\n",
    "\n",
    "# Concat the two dataframes together columnwise\n",
    "final_emp_df = pd.concat([num_emp_df, cat_emp_df], axis=1)\n",
    "\n",
    "print('Dataset dimension after treating categorical features with dummy variables: {} rows, {} columns'.format(final_emp_df.shape[0], final_emp_df.shape[1]))\n",
    "final_emp_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#Model Building and Validation\n",
    "Since, we have to predict a binary class, we will be using classification models for training & predicting Employee Attrition. We need to keep in mind that our focus should be to have a better accuracy of predicting attrition i.e. Attrition = 1 which in confusion matrix will be \"True Positive\". However, we should not forget the prediction accuracy of not qualifying for attrition i.e. Attrition = 0 which will be \"True Negative\" in confusion matrix.\n",
    "\n",
    "So, we need to focus on four parameters:\n",
    "\n",
    "Accuracy: Overall, how often is the classifier correct? i.e {(TP+TN)/Total}\n",
    "True Positive Rate: When it's actually yes, how often does it predict yes? default_ind = 1, {TP/Actual YES}, this is also known as \"Sensitivity\" or \"Recall\"\n",
    "Precision: When it predicts yes, how often is it correct? i.e. {TP/(TP+FP)}\n",
    "Specificity: When it's actually no, how often does it predict no? default_ind = 0, {TN/actual NO}\n",
    "Cross Validation Score: Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. Later, you test your model on this sample before finalizing it. Do this for k folds and take mean of accuracy scores of the k fold models.\n",
    "F1 Score: This is a weighted average of the true positive rate (recall) and precision.\n",
    "ROC Curve: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class.\n",
    "Above information was taken and more details can be found at https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "\n",
    "Step 5.1 - Prepare Train & Test Dataset\n",
    "The data should be divided into train and test data. We will use train_test_split feature to divide the data and we will be using 70-30 ratio\n",
    "\"\"\"\n",
    "# Import the train_test_split method\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Split data into train and test sets as well as for validation and testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(final_emp_df, attrition_target,\n",
    "                                                  test_size= 0.30, random_state=42);\n",
    "\n",
    "print(\"Stratified Sampling: \", len(X_train), \"train set +\", len(X_val), \"validation set\")\n",
    "\n",
    "# Stratified Splitting\n",
    "#split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "#for train_index, test_index in split.split(emp_data_proc, emp_data_proc['Gender']):\n",
    "#    strat_train_set = emp_data_proc.loc[train_index]\n",
    "#    strat_test_set = emp_data_proc.loc[test_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "def gen_model_performance(actual_target, pred_target):\n",
    "    model_conf_matrix = confusion_matrix(actual_target, pred_target)\n",
    "    model_roc_score = roc_auc_score(actual_target, pred_target)\n",
    "    model_accuracy = accuracy_score(actual_target, pred_target) * 100.0\n",
    "    \n",
    "    TP = model_conf_matrix[1][1]; TN = model_conf_matrix[0][0]; \n",
    "    FP = model_conf_matrix[0][1]; FN = model_conf_matrix[1][0];\n",
    "    sensitivity = TP / float(TP + FN) * 100.0; specificity = TN / float(TN + FP) * 100.0;\n",
    "    precision = TP / float(TP + FP) * 100.0;\n",
    "    \n",
    "    return sensitivity, specificity, model_accuracy, precision, model_roc_score\n",
    "def evaluate_model_score(X, y, scoring='accuracy'):\n",
    "    \n",
    "    logreg_model = LogisticRegression(random_state=0)\n",
    "    logreg_cv_model = LogisticRegressionCV()\n",
    "    rfc_model = RandomForestClassifier()\n",
    "    extrees_model = ExtraTreesClassifier()\n",
    "    gboost_model = GradientBoostingClassifier()\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    aboost_model = AdaBoostClassifier()\n",
    "    gnb_model = GaussianNB()\n",
    "\n",
    "    models = [logreg_model, logreg_cv_model, dt_model, rfc_model, \n",
    "              extrees_model, gboost_model, aboost_model, gnb_model]\n",
    "    \n",
    "    model_results = pd.DataFrame(columns = [\"Model\", \"Accuracy\", \"Precision\", \"CV Score\",\n",
    "                                            \"Sensitivity\",\"Specificity\",\"ROC Score\"])\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X, y,)\n",
    "        y_pred = model.predict(X)\n",
    "        score = cross_val_score(model, X, y, cv=5, scoring=scoring)\n",
    "        \n",
    "        sensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y, y_pred)\n",
    "    \n",
    "        scores = cross_val_score(model, X, y, cv=5)\n",
    "    \n",
    "        model_results = model_results.append({\"Model\": model.__class__.__name__,\n",
    "                              \"Accuracy\": accuracy, \"Precision\": precision,\n",
    "                              \"CV Score\": scores.mean()*100.0,\n",
    "                              \"Sensitivity\": sensitivity, \"Specificity\": specificity,\n",
    "                              \"ROC Score\": roc_score}, ignore_index=True)\n",
    "    return model_results\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist=[fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  i.CURRENT_RATING.fillna(i.LAST_RATING,inplace=True)\n",
    "  i.CURRENT_RATING.fillna(i.ALL_PREVIOUS_RAINGS.str[-1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(fd1[fd1.CURRENT_RATING.isnull()].ALL_PREVIOUS_RAING.shape[0]):\n",
    "  a=fd1[fd1.CURRENT_RATING.isnull()].ALL_PREVIOUS_RAING[i]\n",
    "  print(a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist=[fd1,fd2,fd3,fd4,fd5,fd6,fd7,fd8]\n",
    "for i in llist:\n",
    "  i.PRIMARY_SKILL.fillna(i.SECONDARY_SKILL,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''s='20170601'\n",
    "date=pd.to_datetime(s)\n",
    "date'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fd1['PROMOTION_FLAG']=fd1.apply(lambda fd1 : 1 if fd1.LAST_PROMOTION_DATE.year>2016 and fd1.LAST_PROMOTION_DATE.month<6 else 0,axis=1)\n",
    "fd2['PROMOTION_FLAG']=fd2.apply(lambda fd2 : 1 if fd2.LAST_PROMOTION_DATE.year>2016 and fd2.LAST_PROMOTION_DATE.month<10 else 0,axis=1)\n",
    "fd3['PROMOTION_FLAG']=fd3.apply(lambda fd3 : 1 if fd3.LAST_PROMOTION_DATE.year>2016 and fd3.LAST_PROMOTION_DATE.month<12 else 0,axis=1)\n",
    "fd4['PROMOTION_FLAG']=fd4.apply(lambda fd4 : 1 if fd4.LAST_PROMOTION_DATE.year>2017 and fd4.LAST_PROMOTION_DATE.month<3 else 0,axis=1)\n",
    "fd5['PROMOTION_FLAG']=fd5.apply(lambda fd5 : 1 if fd5.LAST_PROMOTION_DATE.year>2017 and fd5.LAST_PROMOTION_DATE.month<6 else 0,axis=1)\n",
    "fd6['PROMOTION_FLAG']=fd6.apply(lambda fd6 : 1 if fd6.LAST_PROMOTION_DATE.year>2017 and fd6.LAST_PROMOTION_DATE.month<10 else 0,axis=1)\n",
    "fd7['PROMOTION_FLAG']=fd7.apply(lambda fd7 : 1 if fd7.LAST_PROMOTION_DATE.year>2017 and fd7.LAST_PROMOTION_DATE.month<12 else 0,axis=1)\n",
    "fd8['PROMOTION_FLAG']=fd8.apply(lambda fd8 : 1 if fd8.LAST_PROMOTION_DATE.year>2018 and fd8.LAST_PROMOTION_DATE.month<3 else 0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "attrition_notebook _train",
  "notebookId": 156868968511199
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
